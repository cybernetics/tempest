{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tempest \u00b6 Typesafe DynamoDB in Kotlin Efficient DynamoDB \u00b6 DynamoDB applications perform best (and cost the least to operate!) when data is organized for locality: Multiple types per table : The application can store different entity types in a single table. DynamoDB schemas are flexible. Related entities are stored together : Entities that are accessed together should be stored together. This makes it possible to answer common queries in as few requests as possible, ideally one . Example \u00b6 Let\u2019s build a music library with the following features: Fetching multiple albums, each of which contains multiple tracks. Fetching individual tracks. We express it like this in Kotlin: interface MusicLibrary { fun getAlbum ( key : AlbumKey ): Album fun getTrack ( key : TrackKey ): Track } data class Album ( val album_title : String , val album_artist : String , val release_date : String , val genre : String , val tracks : List < Track > ) data class Track ( val track_title : String , val run_length : String ) We optimize for this access pattern by putting albums and tracks in the same table: Primary Key Attributes partition_key sort_key ALBUM_1 INFO album_title album_artiest release_date genre The Dark Side of the Moon Pink Floyd 1973-03-01 Progressive rock ALBUM_1 TRACK_1 track_title run_length Speak to Me PT1M13S ALBUM_1 TRACK_2 track_title run_length Breathe PT2M43S ALBUM_1 TRACK_3 track_title run_length On the Run PT3M36S ... ALBUM_2 INFO album_title album_artiest release_date genre The Wall Pink Floyd 1979-11-30 Progressive rock ALBUM_2 TRACK_1 track_title run_length In the Flesh? PT3M20S ... This table uses a composite primary key , (parition_key, sort_key) , to identify each item. The key (\"ALBUM_1\", \"INFO\") identifies ALBUM_1 \u2018s metadata. The key (\"ALBUM_1\", \"TRACK_1\") identifies ALBUM_1 \u2018s first track. This table stores tracks belonging to the same album together and sorts them by the track number. The application needs only one request to DynamoDB to get the album and its tracks. aws dynamodb query \\ --table-name music_library_items \\ --key-conditions '{ \"PK\": { \"ComparisonOperator\": \"EQ\", \"AttributeValueList\": [ { \"S\": \"ALBUM_1\" } ] } }' Why Tempest? \u00b6 For locality, we smashed together several entity types in the same table. This improves performance! But it breaks type safety in DynamoDBMapper. DynamoDBMapper API \u00b6 DynamoDBMapper , the official Java API, forces you to write weakly-typed code that models the actual persistence type. // NOTE: This is not Tempest! It is an example used for comparison. @DynamoDBTable ( tableName = \"music_library_items\" ) class MusicLibraryItem { // All Items. @DynamoDBHashKey var partition_key : String ? = null @DynamoDBRangeKey var sort_key : String ? = null // AlbumInfo. @DynamoDBAttribute var album_title : String ? = null @DynamoDBAttribute var album_artist : String ? = null @DynamoDBAttribute var release_date : String ? = null @DynamoDBAttribute var genre : String ? = null // AlbumTrack. @DynamoDBAttribute var track_title : String ? = null @DynamoDBAttribute var run_length : String ? = null } Note that MusicLibraryItem is a union type of all the entity types: AlbumInfo and AlbumTrack . Because all of its attributes are nullable and mutable, code that interacts with it is brittle and error prone. Tempest API \u00b6 Tempest restores maintainability without losing locality. It lets you declare strongly-typed key and item classes for each logical type in the domain layer. data class AlbumInfo ( @Attribute ( name = \"partition_key\" ) val album_token : String , val album_title : String , val album_artist : String ) { @Attribute ( prefix = \"INFO_\" ) val sort_key : String = \"\" data class Key ( val album_token : String ) { val sort_key : String = \"\" } } data class AlbumTrack ( @Attribute ( name = \"partition_key\" ) val album_token : String , @Attribute ( name = \"sort_key\" , prefix = \"TRACK_\" ) val track_token : String , val track_title : String , val run_length : String ) { data class Key ( val album_token : String , val track_token : String ) } You build business logic with logical types. Tempest handles mapping them to the underlying persistence type. interface MusicLibraryTable : LogicalTable < MusicLibraryItem > { val albumInfo : InlineView < AlbumInfo . Key , AlbumInfo > val albumTracks : InlineView < AlbumTrack . Key , AlbumTrack > } val musicLibrary : MusicLibraryTable // Load. fun getAlbumTitle ( albumToken : String ): String { val key = AlbumInfo . Key ( albumToken ) val info = musicLibrary . albumInfo . load ( key ) return info . album_title } // Update. fun addAlbumTrack ( albumToken : String , track_token : String , track_title : String , run_length : String ) { val newAlbumTrack = AlbumTrack ( albumToken , track_token , track_title , run_length ) musicLibrary . albumTracks . save ( newAlbumTrack ) } // Query. fun getAlbumTrackTitles ( albumToken : String ): List < String > { val albumTracks = musicLibrary . albumTracks . query ( keyCondition = BeginsWith ( AlbumTrack . Key ( albumToken )) ) return albumTracks . map { it . track_title } } Get Tempest \u00b6 With Gradle: implementation \"app.cash.tempest:tempest:0.1.0\" Requirements \u00b6 Tempest builds upon Kotlin\u2019s reflection API and requires types to be declared in Kotlin. License \u00b6 Copyright 2020 Square, Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"#tempest","text":"Typesafe DynamoDB in Kotlin","title":"Tempest"},{"location":"#efficient-dynamodb","text":"DynamoDB applications perform best (and cost the least to operate!) when data is organized for locality: Multiple types per table : The application can store different entity types in a single table. DynamoDB schemas are flexible. Related entities are stored together : Entities that are accessed together should be stored together. This makes it possible to answer common queries in as few requests as possible, ideally one .","title":"Efficient DynamoDB"},{"location":"#example","text":"Let\u2019s build a music library with the following features: Fetching multiple albums, each of which contains multiple tracks. Fetching individual tracks. We express it like this in Kotlin: interface MusicLibrary { fun getAlbum ( key : AlbumKey ): Album fun getTrack ( key : TrackKey ): Track } data class Album ( val album_title : String , val album_artist : String , val release_date : String , val genre : String , val tracks : List < Track > ) data class Track ( val track_title : String , val run_length : String ) We optimize for this access pattern by putting albums and tracks in the same table: Primary Key Attributes partition_key sort_key ALBUM_1 INFO album_title album_artiest release_date genre The Dark Side of the Moon Pink Floyd 1973-03-01 Progressive rock ALBUM_1 TRACK_1 track_title run_length Speak to Me PT1M13S ALBUM_1 TRACK_2 track_title run_length Breathe PT2M43S ALBUM_1 TRACK_3 track_title run_length On the Run PT3M36S ... ALBUM_2 INFO album_title album_artiest release_date genre The Wall Pink Floyd 1979-11-30 Progressive rock ALBUM_2 TRACK_1 track_title run_length In the Flesh? PT3M20S ... This table uses a composite primary key , (parition_key, sort_key) , to identify each item. The key (\"ALBUM_1\", \"INFO\") identifies ALBUM_1 \u2018s metadata. The key (\"ALBUM_1\", \"TRACK_1\") identifies ALBUM_1 \u2018s first track. This table stores tracks belonging to the same album together and sorts them by the track number. The application needs only one request to DynamoDB to get the album and its tracks. aws dynamodb query \\ --table-name music_library_items \\ --key-conditions '{ \"PK\": { \"ComparisonOperator\": \"EQ\", \"AttributeValueList\": [ { \"S\": \"ALBUM_1\" } ] } }'","title":"Example"},{"location":"#why-tempest","text":"For locality, we smashed together several entity types in the same table. This improves performance! But it breaks type safety in DynamoDBMapper.","title":"Why Tempest?"},{"location":"#dynamodbmapper-api","text":"DynamoDBMapper , the official Java API, forces you to write weakly-typed code that models the actual persistence type. // NOTE: This is not Tempest! It is an example used for comparison. @DynamoDBTable ( tableName = \"music_library_items\" ) class MusicLibraryItem { // All Items. @DynamoDBHashKey var partition_key : String ? = null @DynamoDBRangeKey var sort_key : String ? = null // AlbumInfo. @DynamoDBAttribute var album_title : String ? = null @DynamoDBAttribute var album_artist : String ? = null @DynamoDBAttribute var release_date : String ? = null @DynamoDBAttribute var genre : String ? = null // AlbumTrack. @DynamoDBAttribute var track_title : String ? = null @DynamoDBAttribute var run_length : String ? = null } Note that MusicLibraryItem is a union type of all the entity types: AlbumInfo and AlbumTrack . Because all of its attributes are nullable and mutable, code that interacts with it is brittle and error prone.","title":"DynamoDBMapper API"},{"location":"#tempest-api","text":"Tempest restores maintainability without losing locality. It lets you declare strongly-typed key and item classes for each logical type in the domain layer. data class AlbumInfo ( @Attribute ( name = \"partition_key\" ) val album_token : String , val album_title : String , val album_artist : String ) { @Attribute ( prefix = \"INFO_\" ) val sort_key : String = \"\" data class Key ( val album_token : String ) { val sort_key : String = \"\" } } data class AlbumTrack ( @Attribute ( name = \"partition_key\" ) val album_token : String , @Attribute ( name = \"sort_key\" , prefix = \"TRACK_\" ) val track_token : String , val track_title : String , val run_length : String ) { data class Key ( val album_token : String , val track_token : String ) } You build business logic with logical types. Tempest handles mapping them to the underlying persistence type. interface MusicLibraryTable : LogicalTable < MusicLibraryItem > { val albumInfo : InlineView < AlbumInfo . Key , AlbumInfo > val albumTracks : InlineView < AlbumTrack . Key , AlbumTrack > } val musicLibrary : MusicLibraryTable // Load. fun getAlbumTitle ( albumToken : String ): String { val key = AlbumInfo . Key ( albumToken ) val info = musicLibrary . albumInfo . load ( key ) return info . album_title } // Update. fun addAlbumTrack ( albumToken : String , track_token : String , track_title : String , run_length : String ) { val newAlbumTrack = AlbumTrack ( albumToken , track_token , track_title , run_length ) musicLibrary . albumTracks . save ( newAlbumTrack ) } // Query. fun getAlbumTrackTitles ( albumToken : String ): List < String > { val albumTracks = musicLibrary . albumTracks . query ( keyCondition = BeginsWith ( AlbumTrack . Key ( albumToken )) ) return albumTracks . map { it . track_title } }","title":"Tempest API"},{"location":"#get-tempest","text":"With Gradle: implementation \"app.cash.tempest:tempest:0.1.0\"","title":"Get Tempest"},{"location":"#requirements","text":"Tempest builds upon Kotlin\u2019s reflection API and requires types to be declared in Kotlin.","title":"Requirements"},{"location":"#license","text":"Copyright 2020 Square, Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"changelog/","text":"Change Log \u00b6 [0.1.0] - 2020-07-06 \u00b6 Initial release.","title":"Changelog"},{"location":"changelog/#change-log","text":"","title":"Change Log"},{"location":"changelog/#010-2020-07-06","text":"Initial release.","title":"[0.1.0] - 2020-07-06"},{"location":"code_of_conduct/","text":"Open Source Code of Conduct \u00b6 At Square, we are committed to contributing to the open source community and simplifying the process of releasing and managing open source software. We\u2019ve seen incredible support and enthusiasm from thousands of people who have already contributed to our projects\u200a\u2014\u200aand we want to ensure our community continues to be truly open for everyone. This code of conduct outlines our expectations for participants, as well as steps to reporting unacceptable behavior. We are committed to providing a welcoming and inspiring community for all and expect our code of conduct to be honored. Square\u2019s open source community strives to: Be open : We invite anyone to participate in any aspect of our projects. Our community is open, and any responsibility can be carried by a contributor who demonstrates the required capacity and competence. Be considerate : People use our work, and we depend on the work of others. Consider users and colleagues before taking action. For example, changes to code, infrastructure, policy, and documentation may negatively impact others. Be respectful : We expect people to work together to resolve conflict, assume good intentions, and act with empathy. Do not turn disagreements into personal attacks. Be collaborative : Collaboration reduces redundancy and improves the quality of our work. We strive for transparency within our open source community, and we work closely with upstream developers and others in the free software community to coordinate our efforts. Be pragmatic : Questions are encouraged and should be asked early in the process to avoid problems later. Be thoughtful and considerate when seeking out the appropriate forum for your questions. Those who are asked should be responsive and helpful. Step down considerately : Members of every project come and go. When somebody leaves or disengages from the project, they should make it known and take the proper steps to ensure that others can pick up where they left off. This code is not exhaustive or complete. It serves to distill our common understanding of a collaborative, shared environment, and goals. We expect it to be followed in spirit as much as in the letter. Diversity Statement \u00b6 We encourage everyone to participate and are committed to building a community for all. Although we may not be able to satisfy everyone, we all agree that everyone is equal. Whenever a participant has made a mistake, we expect them to take responsibility for it. If someone has been harmed or offended, it is our responsibility to listen carefully and respectfully, and do our best to right the wrong. Although this list cannot be exhaustive, we explicitly honor diversity in age, culture, ethnicity, gender identity or expression, language, national origin, political beliefs, profession, race, religion, sexual orientation, socioeconomic status, and technical ability. We will not tolerate discrimination based on any of the protected characteristics above, including participants with disabilities. Reporting Issues \u00b6 If you experience or witness unacceptable behavior\u200a\u2014\u200aor have any other concerns\u200a\u2014\u200aplease report it by emailing codeofconduct@squareup.com . For more details, please see our Reporting Guidelines below. Thanks \u00b6 Some of the ideas and wording for the statements and guidelines above were based on work by the Twitter , Ubuntu , GDC , and Django communities. We are thankful for their work. Reporting Guide \u00b6 If you experience or witness unacceptable behavior\u200a\u2014\u200aor have any other concerns\u200a\u2014\u200aplease report it by emailing codeofconduct@squareup.com . All reports will be handled with discretion. In your report please include: Your contact information. Names (real, nicknames, or pseudonyms) of any individuals involved. If there are additional witnesses, please include them as well. Your account of what occurred, and if you believe the incident is ongoing. If there is a publicly available record (e.g. a mailing list archive or a public IRC logger), please include a link. Any additional information that may be helpful. After filing a report, a representative from the Square Code of Conduct committee will contact you personally. The committee will then review the incident, follow up with any additional questions, and make a decision as to how to respond. Anyone asked to stop unacceptable behavior is expected to comply immediately. If an individual engages in unacceptable behavior, the Square Code of Conduct committee may take any action they deem appropriate, up to and including a permanent ban from all of Square spaces without warning.","title":"Code of Conduct"},{"location":"code_of_conduct/#open-source-code-of-conduct","text":"At Square, we are committed to contributing to the open source community and simplifying the process of releasing and managing open source software. We\u2019ve seen incredible support and enthusiasm from thousands of people who have already contributed to our projects\u200a\u2014\u200aand we want to ensure our community continues to be truly open for everyone. This code of conduct outlines our expectations for participants, as well as steps to reporting unacceptable behavior. We are committed to providing a welcoming and inspiring community for all and expect our code of conduct to be honored. Square\u2019s open source community strives to: Be open : We invite anyone to participate in any aspect of our projects. Our community is open, and any responsibility can be carried by a contributor who demonstrates the required capacity and competence. Be considerate : People use our work, and we depend on the work of others. Consider users and colleagues before taking action. For example, changes to code, infrastructure, policy, and documentation may negatively impact others. Be respectful : We expect people to work together to resolve conflict, assume good intentions, and act with empathy. Do not turn disagreements into personal attacks. Be collaborative : Collaboration reduces redundancy and improves the quality of our work. We strive for transparency within our open source community, and we work closely with upstream developers and others in the free software community to coordinate our efforts. Be pragmatic : Questions are encouraged and should be asked early in the process to avoid problems later. Be thoughtful and considerate when seeking out the appropriate forum for your questions. Those who are asked should be responsive and helpful. Step down considerately : Members of every project come and go. When somebody leaves or disengages from the project, they should make it known and take the proper steps to ensure that others can pick up where they left off. This code is not exhaustive or complete. It serves to distill our common understanding of a collaborative, shared environment, and goals. We expect it to be followed in spirit as much as in the letter.","title":"Open Source Code of Conduct"},{"location":"code_of_conduct/#diversity-statement","text":"We encourage everyone to participate and are committed to building a community for all. Although we may not be able to satisfy everyone, we all agree that everyone is equal. Whenever a participant has made a mistake, we expect them to take responsibility for it. If someone has been harmed or offended, it is our responsibility to listen carefully and respectfully, and do our best to right the wrong. Although this list cannot be exhaustive, we explicitly honor diversity in age, culture, ethnicity, gender identity or expression, language, national origin, political beliefs, profession, race, religion, sexual orientation, socioeconomic status, and technical ability. We will not tolerate discrimination based on any of the protected characteristics above, including participants with disabilities.","title":"Diversity Statement"},{"location":"code_of_conduct/#reporting-issues","text":"If you experience or witness unacceptable behavior\u200a\u2014\u200aor have any other concerns\u200a\u2014\u200aplease report it by emailing codeofconduct@squareup.com . For more details, please see our Reporting Guidelines below.","title":"Reporting Issues"},{"location":"code_of_conduct/#thanks","text":"Some of the ideas and wording for the statements and guidelines above were based on work by the Twitter , Ubuntu , GDC , and Django communities. We are thankful for their work.","title":"Thanks"},{"location":"code_of_conduct/#reporting-guide","text":"If you experience or witness unacceptable behavior\u200a\u2014\u200aor have any other concerns\u200a\u2014\u200aplease report it by emailing codeofconduct@squareup.com . All reports will be handled with discretion. In your report please include: Your contact information. Names (real, nicknames, or pseudonyms) of any individuals involved. If there are additional witnesses, please include them as well. Your account of what occurred, and if you believe the incident is ongoing. If there is a publicly available record (e.g. a mailing list archive or a public IRC logger), please include a link. Any additional information that may be helpful. After filing a report, a representative from the Square Code of Conduct committee will contact you personally. The committee will then review the incident, follow up with any additional questions, and make a decision as to how to respond. Anyone asked to stop unacceptable behavior is expected to comply immediately. If an individual engages in unacceptable behavior, the Square Code of Conduct committee may take any action they deem appropriate, up to and including a permanent ban from all of Square spaces without warning.","title":"Reporting Guide"},{"location":"contributing/","text":"Contributing \u00b6 If you would like to contribute code to this project you can do so through GitHub by forking the repository and sending a pull request. When submitting code, please make every effort to follow existing conventions and style in order to keep the code as readable as possible. Before your code can be accepted into the project you must also sign the Individual Contributor License Agreement (CLA) .","title":"Contributing"},{"location":"contributing/#contributing","text":"If you would like to contribute code to this project you can do so through GitHub by forking the repository and sending a pull request. When submitting code, please make every effort to follow existing conventions and style in order to keep the code as readable as possible. Before your code can be accepted into the project you must also sign the Individual Contributor License Agreement (CLA) .","title":"Contributing"},{"location":"releasing/","text":"Releasing \u00b6 Change the version in gradle.properties and mkdocs.yml to a non-SNAPSHOT verson. Update the CHANGELOG.md for the impending release. Update the README.md with the new version. git commit -am \"Prepare for release X.Y.Z.\" (where X.Y.Z is the new version) git tag -a X.Y.Z -m \"Version X.Y.Z\" (where X.Y.Z is the new version) Update the gradle.properties to the next SNAPSHOT version. git commit -am \"Prepare next development version.\" git push && git push --tags Wait until the \u201cPublish a release\u201d action completes, then visit Sonatype Nexus and promote the artifacts. Update the sample app to the release version and send a PR. If the github action fails, drop the artifacts from Sonatype and re run the job. You might need to delete the plugin off the JetBrains plugin portal first if the ubuntu job which publishes it already succeeded.","title":"Releasing"},{"location":"releasing/#releasing","text":"Change the version in gradle.properties and mkdocs.yml to a non-SNAPSHOT verson. Update the CHANGELOG.md for the impending release. Update the README.md with the new version. git commit -am \"Prepare for release X.Y.Z.\" (where X.Y.Z is the new version) git tag -a X.Y.Z -m \"Version X.Y.Z\" (where X.Y.Z is the new version) Update the gradle.properties to the next SNAPSHOT version. git commit -am \"Prepare next development version.\" git push && git push --tags Wait until the \u201cPublish a release\u201d action completes, then visit Sonatype Nexus and promote the artifacts. Update the sample app to the release version and send a PR. If the github action fails, drop the artifacts from Sonatype and re run the job. You might need to delete the plugin off the JetBrains plugin portal first if the ubuntu job which publishes it already succeeded.","title":"Releasing"},{"location":"0.x/tempest/","text":"tempest Packages \u00b6 Name Summary app.cash.tempest Index \u00b6 All Types","title":"Packages - Tempest"},{"location":"0.x/tempest/#packages","text":"Name Summary app.cash.tempest","title":"Packages"},{"location":"0.x/tempest/#index","text":"All Types","title":"Index"},{"location":"0.x/tempest/alltypes/","text":"All Types \u00b6 Name Summary app.cash.tempest.Attribute Maps an item class property to one or more attributes in a DynamoDB table. app.cash.tempest.BatchWriteResult It contains the information about the unprocessed items and the exception causing the failure. app.cash.tempest.BatchWriteSet A write that the client sends to the DynamoDb service. app.cash.tempest.BeginsWith Applies equality condition on the hash key and the following condition on the range key app.cash.tempest.Between Applies equality condition on the hash key and the following condition on the range key app.cash.tempest.FilterExpression If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded. app.cash.tempest.ForIndex Maps an key class to a global or local secondary index in a DynamoDB table. app.cash.tempest.InlineView app.cash.tempest.ItemSet A collection of items across tables. app.cash.tempest.KeyCondition Used to query a table or an index. app.cash.tempest.KeySet A collection of keys or items across tables. app.cash.tempest.LogicalDb A collection of tables that implement the DynamoDB best practice of putting multiple item types into the same storage table. This makes it possible to perform aggregate operations and transactions on those item types. app.cash.tempest.LogicalTable A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes. app.cash.tempest.Offset app.cash.tempest.Page app.cash.tempest.Queryable app.cash.tempest.Scannable app.cash.tempest.SecondaryIndex app.cash.tempest.TransactionWriteSet app.cash.tempest.View app.cash.tempest.WorkerId By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. app.cash.tempest.WritingPager A control flow abstraction for paging transactional writes.","title":"All Types - Tempest"},{"location":"0.x/tempest/alltypes/#all-types","text":"Name Summary app.cash.tempest.Attribute Maps an item class property to one or more attributes in a DynamoDB table. app.cash.tempest.BatchWriteResult It contains the information about the unprocessed items and the exception causing the failure. app.cash.tempest.BatchWriteSet A write that the client sends to the DynamoDb service. app.cash.tempest.BeginsWith Applies equality condition on the hash key and the following condition on the range key app.cash.tempest.Between Applies equality condition on the hash key and the following condition on the range key app.cash.tempest.FilterExpression If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded. app.cash.tempest.ForIndex Maps an key class to a global or local secondary index in a DynamoDB table. app.cash.tempest.InlineView app.cash.tempest.ItemSet A collection of items across tables. app.cash.tempest.KeyCondition Used to query a table or an index. app.cash.tempest.KeySet A collection of keys or items across tables. app.cash.tempest.LogicalDb A collection of tables that implement the DynamoDB best practice of putting multiple item types into the same storage table. This makes it possible to perform aggregate operations and transactions on those item types. app.cash.tempest.LogicalTable A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes. app.cash.tempest.Offset app.cash.tempest.Page app.cash.tempest.Queryable app.cash.tempest.Scannable app.cash.tempest.SecondaryIndex app.cash.tempest.TransactionWriteSet app.cash.tempest.View app.cash.tempest.WorkerId By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. app.cash.tempest.WritingPager A control flow abstraction for paging transactional writes.","title":"All Types"},{"location":"0.x/tempest/app.cash.tempest/","text":"tempest / app.cash.tempest Package app.cash.tempest \u00b6 Types \u00b6 Name Summary BatchWriteResult data class BatchWriteResult It contains the information about the unprocessed items and the exception causing the failure. BatchWriteSet data class BatchWriteSet A write that the client sends to the DynamoDb service. BeginsWith data class BeginsWith<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key Between data class Between<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key FilterExpression data class FilterExpression If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded. InlineView interface InlineView<K : Any , I : Any > : View < K , I >, Scannable < K , I >, Queryable < K , I > ItemSet class ItemSet : Set < Any > A collection of items across tables. KeyCondition sealed class KeyCondition<K : Any > Used to query a table or an index. KeySet class KeySet : Set < Any > A collection of keys or items across tables. LogicalDb interface LogicalDb : LogicalTable.Factory A collection of tables that implement the DynamoDB best practice of putting multiple item types into the same storage table. This makes it possible to perform aggregate operations and transactions on those item types. LogicalTable interface LogicalTable<RI : Any > : View < RI , RI >, InlineView.Factory , SecondaryIndex.Factory A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes. Offset data class Offset<K> Page data class Page<K, T> Queryable interface Queryable<K : Any , I : Any > Scannable interface Scannable<K : Any , I : Any > SecondaryIndex interface SecondaryIndex<K : Any , I : Any > : Scannable < K , I >, Queryable < K , I > TransactionWriteSet data class TransactionWriteSet View interface View<K : Any , I : Any > WorkerId data class WorkerId By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. WritingPager class WritingPager<T> A control flow abstraction for paging transactional writes. Annotations \u00b6 Name Summary Attribute annotation class Attribute Maps an item class property to one or more attributes in a DynamoDB table. ForIndex annotation class ForIndex Maps an key class to a global or local secondary index in a DynamoDB table. Functions \u00b6 Name Summary transactionWritingPager fun <DB : LogicalDb , T> DB .transactionWritingPager(items: List < T >, maxTransactionItems: Int = 25, handler: WritingPager.Handler < T >): WritingPager < T >","title":"0.x API"},{"location":"0.x/tempest/app.cash.tempest/#package-appcashtempest","text":"","title":"Package app.cash.tempest"},{"location":"0.x/tempest/app.cash.tempest/#types","text":"Name Summary BatchWriteResult data class BatchWriteResult It contains the information about the unprocessed items and the exception causing the failure. BatchWriteSet data class BatchWriteSet A write that the client sends to the DynamoDb service. BeginsWith data class BeginsWith<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key Between data class Between<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key FilterExpression data class FilterExpression If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded. InlineView interface InlineView<K : Any , I : Any > : View < K , I >, Scannable < K , I >, Queryable < K , I > ItemSet class ItemSet : Set < Any > A collection of items across tables. KeyCondition sealed class KeyCondition<K : Any > Used to query a table or an index. KeySet class KeySet : Set < Any > A collection of keys or items across tables. LogicalDb interface LogicalDb : LogicalTable.Factory A collection of tables that implement the DynamoDB best practice of putting multiple item types into the same storage table. This makes it possible to perform aggregate operations and transactions on those item types. LogicalTable interface LogicalTable<RI : Any > : View < RI , RI >, InlineView.Factory , SecondaryIndex.Factory A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes. Offset data class Offset<K> Page data class Page<K, T> Queryable interface Queryable<K : Any , I : Any > Scannable interface Scannable<K : Any , I : Any > SecondaryIndex interface SecondaryIndex<K : Any , I : Any > : Scannable < K , I >, Queryable < K , I > TransactionWriteSet data class TransactionWriteSet View interface View<K : Any , I : Any > WorkerId data class WorkerId By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. WritingPager class WritingPager<T> A control flow abstraction for paging transactional writes.","title":"Types"},{"location":"0.x/tempest/app.cash.tempest/#annotations","text":"Name Summary Attribute annotation class Attribute Maps an item class property to one or more attributes in a DynamoDB table. ForIndex annotation class ForIndex Maps an key class to a global or local secondary index in a DynamoDB table.","title":"Annotations"},{"location":"0.x/tempest/app.cash.tempest/#functions","text":"Name Summary transactionWritingPager fun <DB : LogicalDb , T> DB .transactionWritingPager(items: List < T >, maxTransactionItems: Int = 25, handler: WritingPager.Handler < T >): WritingPager < T >","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-key-condition/","text":"tempest / app.cash.tempest / KeyCondition KeyCondition \u00b6 sealed class KeyCondition<K : Any > Used to query a table or an index. Inheritors \u00b6 Name Summary BeginsWith data class BeginsWith<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key Between data class Between<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key","title":"KeyCondition - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-key-condition/#keycondition","text":"sealed class KeyCondition<K : Any > Used to query a table or an index.","title":"KeyCondition"},{"location":"0.x/tempest/app.cash.tempest/-key-condition/#inheritors","text":"Name Summary BeginsWith data class BeginsWith<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key Between data class Between<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key","title":"Inheritors"},{"location":"0.x/tempest/app.cash.tempest/transaction-writing-pager/","text":"tempest / app.cash.tempest / transactionWritingPager transactionWritingPager \u00b6 fun <DB : LogicalDb , T> DB .transactionWritingPager(items: List < T >, maxTransactionItems: Int = 25, handler: WritingPager.Handler < T >): WritingPager < T >","title":"transactionWritingPager - Tempest"},{"location":"0.x/tempest/app.cash.tempest/transaction-writing-pager/#transactionwritingpager","text":"fun <DB : LogicalDb , T> DB .transactionWritingPager(items: List < T >, maxTransactionItems: Int = 25, handler: WritingPager.Handler < T >): WritingPager < T >","title":"transactionWritingPager"},{"location":"0.x/tempest/app.cash.tempest/-attribute/","text":"tempest / app.cash.tempest / Attribute Attribute \u00b6 annotation class Attribute Maps an item class property to one or more attributes in a DynamoDB table. If this mapped to a primary range key, it must have a prefix. Constructors \u00b6 Name Summary <init> Attribute(name: String = \"\", names: Array < String > = [], prefix: String = \"\") Maps an item class property to one or more attributes in a DynamoDB table. Properties \u00b6 Name Summary name val name: String names val names: Array < String > prefix val prefix: String","title":"Attribute - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-attribute/#attribute","text":"annotation class Attribute Maps an item class property to one or more attributes in a DynamoDB table. If this mapped to a primary range key, it must have a prefix.","title":"Attribute"},{"location":"0.x/tempest/app.cash.tempest/-attribute/#constructors","text":"Name Summary <init> Attribute(name: String = \"\", names: Array < String > = [], prefix: String = \"\") Maps an item class property to one or more attributes in a DynamoDB table.","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-attribute/#properties","text":"Name Summary name val name: String names val names: Array < String > prefix val prefix: String","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-attribute/-init-/","text":"tempest / app.cash.tempest / Attribute / <init> <init> \u00b6 Attribute(name: String = \"\", names: Array < String > = [], prefix: String = \"\") Maps an item class property to one or more attributes in a DynamoDB table. If this mapped to a primary range key, it must have a prefix.","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-attribute/-init-/#init","text":"Attribute(name: String = \"\", names: Array < String > = [], prefix: String = \"\") Maps an item class property to one or more attributes in a DynamoDB table. If this mapped to a primary range key, it must have a prefix.","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-attribute/name/","text":"tempest / app.cash.tempest / Attribute / name name \u00b6 val name: String","title":"name - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-attribute/name/#name","text":"val name: String","title":"name"},{"location":"0.x/tempest/app.cash.tempest/-attribute/names/","text":"tempest / app.cash.tempest / Attribute / names names \u00b6 val names: Array < String >","title":"names - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-attribute/names/#names","text":"val names: Array < String >","title":"names"},{"location":"0.x/tempest/app.cash.tempest/-attribute/prefix/","text":"tempest / app.cash.tempest / Attribute / prefix prefix \u00b6 val prefix: String","title":"prefix - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-attribute/prefix/#prefix","text":"val prefix: String","title":"prefix"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-result/","text":"tempest / app.cash.tempest / BatchWriteResult BatchWriteResult \u00b6 data class BatchWriteResult It contains the information about the unprocessed items and the exception causing the failure. Constructors \u00b6 Name Summary <init> BatchWriteResult(failedBatches: List < FailedBatch >) It contains the information about the unprocessed items and the exception causing the failure. Properties \u00b6 Name Summary failedBatches val failedBatches: List < FailedBatch > isSuccessful val isSuccessful: Boolean","title":"BatchWriteResult - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-result/#batchwriteresult","text":"data class BatchWriteResult It contains the information about the unprocessed items and the exception causing the failure.","title":"BatchWriteResult"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-result/#constructors","text":"Name Summary <init> BatchWriteResult(failedBatches: List < FailedBatch >) It contains the information about the unprocessed items and the exception causing the failure.","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-result/#properties","text":"Name Summary failedBatches val failedBatches: List < FailedBatch > isSuccessful val isSuccessful: Boolean","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-result/-init-/","text":"tempest / app.cash.tempest / BatchWriteResult / <init> <init> \u00b6 BatchWriteResult(failedBatches: List < FailedBatch >) It contains the information about the unprocessed items and the exception causing the failure.","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-batch-write-result/-init-/#init","text":"BatchWriteResult(failedBatches: List < FailedBatch >) It contains the information about the unprocessed items and the exception causing the failure.","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-result/failed-batches/","text":"tempest / app.cash.tempest / BatchWriteResult / failedBatches failedBatches \u00b6 val failedBatches: List < FailedBatch >","title":"failedBatches - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-result/failed-batches/#failedbatches","text":"val failedBatches: List < FailedBatch >","title":"failedBatches"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-result/is-successful/","text":"tempest / app.cash.tempest / BatchWriteResult / isSuccessful isSuccessful \u00b6 val isSuccessful: Boolean","title":"isSuccessful - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-result/is-successful/#issuccessful","text":"val isSuccessful: Boolean","title":"isSuccessful"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/","text":"tempest / app.cash.tempest / BatchWriteSet BatchWriteSet \u00b6 data class BatchWriteSet A write that the client sends to the DynamoDb service. Types \u00b6 Name Summary Builder class Builder Constructors \u00b6 Name Summary <init> BatchWriteSet(itemsToClobber: ItemSet , keysToDelete: KeySet ) A write that the client sends to the DynamoDb service. Properties \u00b6 Name Summary itemsToClobber val itemsToClobber: ItemSet keysToDelete val keysToDelete: KeySet","title":"BatchWriteSet - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/#batchwriteset","text":"data class BatchWriteSet A write that the client sends to the DynamoDb service.","title":"BatchWriteSet"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/#types","text":"Name Summary Builder class Builder","title":"Types"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/#constructors","text":"Name Summary <init> BatchWriteSet(itemsToClobber: ItemSet , keysToDelete: KeySet ) A write that the client sends to the DynamoDb service.","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/#properties","text":"Name Summary itemsToClobber val itemsToClobber: ItemSet keysToDelete val keysToDelete: KeySet","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-init-/","text":"tempest / app.cash.tempest / BatchWriteSet / <init> <init> \u00b6 BatchWriteSet(itemsToClobber: ItemSet , keysToDelete: KeySet ) A write that the client sends to the DynamoDb service.","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-init-/#init","text":"BatchWriteSet(itemsToClobber: ItemSet , keysToDelete: KeySet ) A write that the client sends to the DynamoDb service.","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/items-to-clobber/","text":"tempest / app.cash.tempest / BatchWriteSet / itemsToClobber itemsToClobber \u00b6 val itemsToClobber: ItemSet","title":"itemsToClobber - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/items-to-clobber/#itemstoclobber","text":"val itemsToClobber: ItemSet","title":"itemsToClobber"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/keys-to-delete/","text":"tempest / app.cash.tempest / BatchWriteSet / keysToDelete keysToDelete \u00b6 val keysToDelete: KeySet","title":"keysToDelete - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/keys-to-delete/#keystodelete","text":"val keysToDelete: KeySet","title":"keysToDelete"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/","text":"tempest / app.cash.tempest / BatchWriteSet / Builder Builder \u00b6 class Builder Constructors \u00b6 Name Summary <init> Builder() Functions \u00b6 Name Summary build fun build(): BatchWriteSet clobber fun clobber(vararg item: Any ): BatchWriteSet.Builder This method behaves as if SaveBehavior.CLOBBER was specified. Versioned attributes will be discarded. fun clobber(items: Iterable < Any >): BatchWriteSet.Builder delete fun delete(vararg key: Any ): BatchWriteSet.Builder fun delete(keys: Iterable < Any >): BatchWriteSet.Builder","title":"Builder - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/#builder","text":"class Builder","title":"Builder"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/#constructors","text":"Name Summary <init> Builder()","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/#functions","text":"Name Summary build fun build(): BatchWriteSet clobber fun clobber(vararg item: Any ): BatchWriteSet.Builder This method behaves as if SaveBehavior.CLOBBER was specified. Versioned attributes will be discarded. fun clobber(items: Iterable < Any >): BatchWriteSet.Builder delete fun delete(vararg key: Any ): BatchWriteSet.Builder fun delete(keys: Iterable < Any >): BatchWriteSet.Builder","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/-init-/","text":"tempest / app.cash.tempest / BatchWriteSet / Builder / <init> <init> \u00b6 Builder()","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/-init-/#init","text":"Builder()","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/build/","text":"tempest / app.cash.tempest / BatchWriteSet / Builder / build build \u00b6 fun build(): BatchWriteSet","title":"build - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/build/#build","text":"fun build(): BatchWriteSet","title":"build"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/clobber/","text":"tempest / app.cash.tempest / BatchWriteSet / Builder / clobber clobber \u00b6 fun clobber(vararg item: Any ): BatchWriteSet.Builder This method behaves as if SaveBehavior.CLOBBER was specified. Versioned attributes will be discarded. fun clobber(items: Iterable < Any >): BatchWriteSet.Builder","title":"clobber - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/clobber/#clobber","text":"fun clobber(vararg item: Any ): BatchWriteSet.Builder This method behaves as if SaveBehavior.CLOBBER was specified. Versioned attributes will be discarded. fun clobber(items: Iterable < Any >): BatchWriteSet.Builder","title":"clobber"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/delete/","text":"tempest / app.cash.tempest / BatchWriteSet / Builder / delete delete \u00b6 fun delete(vararg key: Any ): BatchWriteSet.Builder fun delete(keys: Iterable < Any >): BatchWriteSet.Builder","title":"delete - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-batch-write-set/-builder/delete/#delete","text":"fun delete(vararg key: Any ): BatchWriteSet.Builder fun delete(keys: Iterable < Any >): BatchWriteSet.Builder","title":"delete"},{"location":"0.x/tempest/app.cash.tempest/-begins-with/","text":"tempest / app.cash.tempest / BeginsWith BeginsWith \u00b6 data class BeginsWith<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key begins_with (a, substr)\u2014 true if the value of attribute a begins with a particular substring. Constructors \u00b6 Name Summary <init> BeginsWith(prefix: K ) Applies equality condition on the hash key and the following condition on the range key Properties \u00b6 Name Summary prefix val prefix: K","title":"BeginsWith - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-begins-with/#beginswith","text":"data class BeginsWith<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key begins_with (a, substr)\u2014 true if the value of attribute a begins with a particular substring.","title":"BeginsWith"},{"location":"0.x/tempest/app.cash.tempest/-begins-with/#constructors","text":"Name Summary <init> BeginsWith(prefix: K ) Applies equality condition on the hash key and the following condition on the range key","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-begins-with/#properties","text":"Name Summary prefix val prefix: K","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-begins-with/-init-/","text":"tempest / app.cash.tempest / BeginsWith / <init> <init> \u00b6 BeginsWith(prefix: K ) Applies equality condition on the hash key and the following condition on the range key begins_with (a, substr)\u2014 true if the value of attribute a begins with a particular substring.","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-begins-with/-init-/#init","text":"BeginsWith(prefix: K ) Applies equality condition on the hash key and the following condition on the range key begins_with (a, substr)\u2014 true if the value of attribute a begins with a particular substring.","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-begins-with/prefix/","text":"tempest / app.cash.tempest / BeginsWith / prefix prefix \u00b6 val prefix: K","title":"prefix - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-begins-with/prefix/#prefix","text":"val prefix: K","title":"prefix"},{"location":"0.x/tempest/app.cash.tempest/-between/","text":"tempest / app.cash.tempest / Between Between \u00b6 data class Between<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key a BETWEEN b AND c \u2014 true if a is greater than or equal to b, and less than or equal to c. Constructors \u00b6 Name Summary <init> Between(startInclusive: K , endInclusive: K ) Applies equality condition on the hash key and the following condition on the range key Properties \u00b6 Name Summary endInclusive val endInclusive: K startInclusive val startInclusive: K","title":"Between - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-between/#between","text":"data class Between<K : Any > : KeyCondition < K > Applies equality condition on the hash key and the following condition on the range key a BETWEEN b AND c \u2014 true if a is greater than or equal to b, and less than or equal to c.","title":"Between"},{"location":"0.x/tempest/app.cash.tempest/-between/#constructors","text":"Name Summary <init> Between(startInclusive: K , endInclusive: K ) Applies equality condition on the hash key and the following condition on the range key","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-between/#properties","text":"Name Summary endInclusive val endInclusive: K startInclusive val startInclusive: K","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-between/-init-/","text":"tempest / app.cash.tempest / Between / <init> <init> \u00b6 Between(startInclusive: K , endInclusive: K ) Applies equality condition on the hash key and the following condition on the range key a BETWEEN b AND c \u2014 true if a is greater than or equal to b, and less than or equal to c.","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-between/-init-/#init","text":"Between(startInclusive: K , endInclusive: K ) Applies equality condition on the hash key and the following condition on the range key a BETWEEN b AND c \u2014 true if a is greater than or equal to b, and less than or equal to c.","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-between/end-inclusive/","text":"tempest / app.cash.tempest / Between / endInclusive endInclusive \u00b6 val endInclusive: K","title":"endInclusive - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-between/end-inclusive/#endinclusive","text":"val endInclusive: K","title":"endInclusive"},{"location":"0.x/tempest/app.cash.tempest/-between/start-inclusive/","text":"tempest / app.cash.tempest / Between / startInclusive startInclusive \u00b6 val startInclusive: K","title":"startInclusive - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-between/start-inclusive/#startinclusive","text":"val startInclusive: K","title":"startInclusive"},{"location":"0.x/tempest/app.cash.tempest/-filter-expression/","text":"tempest / app.cash.tempest / FilterExpression FilterExpression \u00b6 data class FilterExpression If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded. A filter expression is applied after a Scan finishes but before the results are returned. Therefore, a Scan consumes the same amount of read capacity, regardless of whether a filter expression is present. Constructors \u00b6 Name Summary <init> FilterExpression(expression: String , attributeValues: Map < String , AttributeValue > = emptyMap()) If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded. Properties \u00b6 Name Summary attributeValues val attributeValues: Map < String , AttributeValue > Expression attribute values in Amazon DynamoDB are substitutes for the actual values that you want to compare\u2014values that you might not know until runtime. An expression attribute value must begin with a colon (:) and be followed by one or more alphanumeric characters. expression val expression: String The syntax for a filter expression is identical to that of a condition expression. Filter expressions can use the same comparators, functions, and logical operators as a condition expression. For more information, Condition Expressions .","title":"FilterExpression - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-filter-expression/#filterexpression","text":"data class FilterExpression If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded. A filter expression is applied after a Scan finishes but before the results are returned. Therefore, a Scan consumes the same amount of read capacity, regardless of whether a filter expression is present.","title":"FilterExpression"},{"location":"0.x/tempest/app.cash.tempest/-filter-expression/#constructors","text":"Name Summary <init> FilterExpression(expression: String , attributeValues: Map < String , AttributeValue > = emptyMap()) If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded.","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-filter-expression/#properties","text":"Name Summary attributeValues val attributeValues: Map < String , AttributeValue > Expression attribute values in Amazon DynamoDB are substitutes for the actual values that you want to compare\u2014values that you might not know until runtime. An expression attribute value must begin with a colon (:) and be followed by one or more alphanumeric characters. expression val expression: String The syntax for a filter expression is identical to that of a condition expression. Filter expressions can use the same comparators, functions, and logical operators as a condition expression. For more information, Condition Expressions .","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-filter-expression/-init-/","text":"tempest / app.cash.tempest / FilterExpression / <init> <init> \u00b6 FilterExpression(expression: String , attributeValues: Map < String , AttributeValue > = emptyMap()) If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded. A filter expression is applied after a Scan finishes but before the results are returned. Therefore, a Scan consumes the same amount of read capacity, regardless of whether a filter expression is present.","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-filter-expression/-init-/#init","text":"FilterExpression(expression: String , attributeValues: Map < String , AttributeValue > = emptyMap()) If you need to further refine the Scan results, you can optionally provide a filter expression. A filter expression determines which items within the Scan results should be returned to you. All of the other results are discarded. A filter expression is applied after a Scan finishes but before the results are returned. Therefore, a Scan consumes the same amount of read capacity, regardless of whether a filter expression is present.","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-filter-expression/attribute-values/","text":"tempest / app.cash.tempest / FilterExpression / attributeValues attributeValues \u00b6 val attributeValues: Map < String , AttributeValue > Expression attribute values in Amazon DynamoDB are substitutes for the actual values that you want to compare\u2014values that you might not know until runtime. An expression attribute value must begin with a colon (:) and be followed by one or more alphanumeric characters.","title":"attributeValues - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-filter-expression/attribute-values/#attributevalues","text":"val attributeValues: Map < String , AttributeValue > Expression attribute values in Amazon DynamoDB are substitutes for the actual values that you want to compare\u2014values that you might not know until runtime. An expression attribute value must begin with a colon (:) and be followed by one or more alphanumeric characters.","title":"attributeValues"},{"location":"0.x/tempest/app.cash.tempest/-filter-expression/expression/","text":"tempest / app.cash.tempest / FilterExpression / expression expression \u00b6 val expression: String The syntax for a filter expression is identical to that of a condition expression. Filter expressions can use the same comparators, functions, and logical operators as a condition expression. For more information, Condition Expressions .","title":"expression - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-filter-expression/expression/#expression","text":"val expression: String The syntax for a filter expression is identical to that of a condition expression. Filter expressions can use the same comparators, functions, and logical operators as a condition expression. For more information, Condition Expressions .","title":"expression"},{"location":"0.x/tempest/app.cash.tempest/-for-index/","text":"tempest / app.cash.tempest / ForIndex ForIndex \u00b6 annotation class ForIndex Maps an key class to a global or local secondary index in a DynamoDB table. Constructors \u00b6 Name Summary <init> ForIndex(name: String = \"\") Maps an key class to a global or local secondary index in a DynamoDB table. Properties \u00b6 Name Summary name val name: String","title":"ForIndex - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-for-index/#forindex","text":"annotation class ForIndex Maps an key class to a global or local secondary index in a DynamoDB table.","title":"ForIndex"},{"location":"0.x/tempest/app.cash.tempest/-for-index/#constructors","text":"Name Summary <init> ForIndex(name: String = \"\") Maps an key class to a global or local secondary index in a DynamoDB table.","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-for-index/#properties","text":"Name Summary name val name: String","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-for-index/-init-/","text":"tempest / app.cash.tempest / ForIndex / <init> <init> \u00b6 ForIndex(name: String = \"\") Maps an key class to a global or local secondary index in a DynamoDB table.","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-for-index/-init-/#init","text":"ForIndex(name: String = \"\") Maps an key class to a global or local secondary index in a DynamoDB table.","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-for-index/name/","text":"tempest / app.cash.tempest / ForIndex / name name \u00b6 val name: String","title":"name - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-for-index/name/#name","text":"val name: String","title":"name"},{"location":"0.x/tempest/app.cash.tempest/-inline-view/","text":"tempest / app.cash.tempest / InlineView InlineView \u00b6 interface InlineView<K : Any , I : Any > : View < K , I >, Scannable < K , I >, Queryable < K , I > Types \u00b6 Name Summary Factory interface Factory Inherited Functions \u00b6 Name Summary delete abstract fun delete(item: I , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes item from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. deleteKey abstract fun deleteKey(key: K , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes the item identified by key from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. load abstract fun load(key: K , consistentReads: ConsistentReads = ConsistentReads.EVENTUAL): I ? Returns an item whose keys match those of the prototype key object given, or null if no such item exists. query abstract fun query(keyCondition: KeyCondition < K >, consistentRead: Boolean = false, asc: Boolean = true, pageSize: Int = 100, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Reads up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated. save abstract fun save(item: I , saveExpression: DynamoDBSaveExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Saves an item in DynamoDB. This method uses DynamoDBMapperConfig.SaveBehavior.PUT to clear and replace all attributes, including unmodeled ones, on save. Partial update, i.e. DynamoDBMapperConfig.SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES , is not supported yet. scan abstract fun scan(workerId: WorkerId = WorkerId.SEQUENTIAL, pageSize: Int = 100, consistentRead: Boolean = false, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Scans up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.","title":"InlineView - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-inline-view/#inlineview","text":"interface InlineView<K : Any , I : Any > : View < K , I >, Scannable < K , I >, Queryable < K , I >","title":"InlineView"},{"location":"0.x/tempest/app.cash.tempest/-inline-view/#types","text":"Name Summary Factory interface Factory","title":"Types"},{"location":"0.x/tempest/app.cash.tempest/-inline-view/#inherited-functions","text":"Name Summary delete abstract fun delete(item: I , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes item from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. deleteKey abstract fun deleteKey(key: K , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes the item identified by key from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. load abstract fun load(key: K , consistentReads: ConsistentReads = ConsistentReads.EVENTUAL): I ? Returns an item whose keys match those of the prototype key object given, or null if no such item exists. query abstract fun query(keyCondition: KeyCondition < K >, consistentRead: Boolean = false, asc: Boolean = true, pageSize: Int = 100, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Reads up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated. save abstract fun save(item: I , saveExpression: DynamoDBSaveExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Saves an item in DynamoDB. This method uses DynamoDBMapperConfig.SaveBehavior.PUT to clear and replace all attributes, including unmodeled ones, on save. Partial update, i.e. DynamoDBMapperConfig.SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES , is not supported yet. scan abstract fun scan(workerId: WorkerId = WorkerId.SEQUENTIAL, pageSize: Int = 100, consistentRead: Boolean = false, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Scans up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.","title":"Inherited Functions"},{"location":"0.x/tempest/app.cash.tempest/-inline-view/-factory/","text":"tempest / app.cash.tempest / InlineView / Factory Factory \u00b6 interface Factory Functions \u00b6 Name Summary inlineView abstract fun <K : Any , I : Any > inlineView(keyType: KClass < K >, itemType: KClass < I >): InlineView < K , I > Inheritors \u00b6 Name Summary LogicalTable interface LogicalTable<RI : Any > : View < RI , RI >, InlineView.Factory , SecondaryIndex.Factory A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes.","title":"Factory - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-inline-view/-factory/#factory","text":"interface Factory","title":"Factory"},{"location":"0.x/tempest/app.cash.tempest/-inline-view/-factory/#functions","text":"Name Summary inlineView abstract fun <K : Any , I : Any > inlineView(keyType: KClass < K >, itemType: KClass < I >): InlineView < K , I >","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-inline-view/-factory/#inheritors","text":"Name Summary LogicalTable interface LogicalTable<RI : Any > : View < RI , RI >, InlineView.Factory , SecondaryIndex.Factory A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes.","title":"Inheritors"},{"location":"0.x/tempest/app.cash.tempest/-inline-view/-factory/inline-view/","text":"tempest / app.cash.tempest / InlineView / Factory / inlineView inlineView \u00b6 abstract fun <K : Any , I : Any > inlineView(keyType: KClass < K >, itemType: KClass < I >): InlineView < K , I >","title":"inlineView - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-inline-view/-factory/inline-view/#inlineview","text":"abstract fun <K : Any , I : Any > inlineView(keyType: KClass < K >, itemType: KClass < I >): InlineView < K , I >","title":"inlineView"},{"location":"0.x/tempest/app.cash.tempest/-item-set/","text":"tempest / app.cash.tempest / ItemSet ItemSet \u00b6 class ItemSet : Set < Any > A collection of items across tables. Constructors \u00b6 Name Summary <init> ItemSet(contents: Iterable < Any >) Functions \u00b6 Name Summary getItems fun <I : Any > getItems(itemType: KClass < I >): Set < I > fun <I : Any > getItems(): Collection < I >","title":"ItemSet - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-item-set/#itemset","text":"class ItemSet : Set < Any > A collection of items across tables.","title":"ItemSet"},{"location":"0.x/tempest/app.cash.tempest/-item-set/#constructors","text":"Name Summary <init> ItemSet(contents: Iterable < Any >)","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-item-set/#functions","text":"Name Summary getItems fun <I : Any > getItems(itemType: KClass < I >): Set < I > fun <I : Any > getItems(): Collection < I >","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-item-set/-init-/","text":"tempest / app.cash.tempest / ItemSet / <init> <init> \u00b6 ItemSet(contents: Iterable < Any >)","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-item-set/-init-/#init","text":"ItemSet(contents: Iterable < Any >)","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-item-set/get-items/","text":"tempest / app.cash.tempest / ItemSet / getItems getItems \u00b6 fun <I : Any > getItems(itemType: KClass < I >): Set < I > inline fun <reified I : Any > getItems(): Collection < I >","title":"getItems - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-item-set/get-items/#getitems","text":"fun <I : Any > getItems(itemType: KClass < I >): Set < I > inline fun <reified I : Any > getItems(): Collection < I >","title":"getItems"},{"location":"0.x/tempest/app.cash.tempest/-key-set/","text":"tempest / app.cash.tempest / KeySet KeySet \u00b6 class KeySet : Set < Any > A collection of keys or items across tables. Constructors \u00b6 Name Summary <init> KeySet(contents: Iterable < Any >) Functions \u00b6 Name Summary getKeys fun <K : Any > getKeys(keyType: KClass < K >): Set < K > fun <K : Any > getKeys(): Collection < K >","title":"KeySet - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-key-set/#keyset","text":"class KeySet : Set < Any > A collection of keys or items across tables.","title":"KeySet"},{"location":"0.x/tempest/app.cash.tempest/-key-set/#constructors","text":"Name Summary <init> KeySet(contents: Iterable < Any >)","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-key-set/#functions","text":"Name Summary getKeys fun <K : Any > getKeys(keyType: KClass < K >): Set < K > fun <K : Any > getKeys(): Collection < K >","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-key-set/-init-/","text":"tempest / app.cash.tempest / KeySet / <init> <init> \u00b6 KeySet(contents: Iterable < Any >)","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-key-set/-init-/#init","text":"KeySet(contents: Iterable < Any >)","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-key-set/get-keys/","text":"tempest / app.cash.tempest / KeySet / getKeys getKeys \u00b6 fun <K : Any > getKeys(keyType: KClass < K >): Set < K > inline fun <reified K : Any > getKeys(): Collection < K >","title":"getKeys - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-key-set/get-keys/#getkeys","text":"fun <K : Any > getKeys(keyType: KClass < K >): Set < K > inline fun <reified K : Any > getKeys(): Collection < K >","title":"getKeys"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/","text":"tempest / app.cash.tempest / LogicalDb LogicalDb \u00b6 interface LogicalDb : LogicalTable.Factory A collection of tables that implement the DynamoDB best practice of putting multiple item types into the same storage table. This makes it possible to perform aggregate operations and transactions on those item types. Functions \u00b6 Name Summary batchLoad abstract fun batchLoad(keys: KeySet , consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet Retrieves multiple items from multiple tables using their primary keys. open fun batchLoad(keys: Iterable < Any >, consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet open fun batchLoad(vararg keys: Any , consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet batchWrite abstract fun batchWrite(writeSet: BatchWriteSet , retryStrategy: BatchWriteRetryStrategy = DefaultBatchWriteRetryStrategy()): BatchWriteResult Saves and deletes the objects given using one or more calls to the AmazonDynamoDB.batchWriteItem API. Callers should always check the returned BatchWriteResult because this method returns normally even if some writes were not performed. transactionLoad abstract fun transactionLoad(keys: KeySet ): ItemSet Transactionally loads objects specified by transactionLoadRequest by calling AmazonDynamoDB.transactGetItems API. open fun transactionLoad(keys: Iterable < Any >): ItemSet open fun transactionLoad(vararg keys: Any ): ItemSet transactionWrite abstract fun transactionWrite(writeSet: TransactionWriteSet ): Unit Transactionally writes objects specified by transactionWriteRequest by calling AmazonDynamoDB.transactWriteItems API. Inherited Functions \u00b6 Name Summary logicalTable abstract fun <T : LogicalTable < RI >, RI : Any > logicalTable(tableType: KClass < T >): T Companion Object Functions \u00b6 Name Summary create fun <DB : LogicalDb > create(dbType: KClass < DB >, dynamoDbMapper: DynamoDBMapper , config: DynamoDBMapperConfig = DynamoDBMapperConfig.DEFAULT): DB invoke operator fun <DB : LogicalDb > invoke(dynamoDbMapper: DynamoDBMapper , config: DynamoDBMapperConfig = DynamoDBMapperConfig.DEFAULT): DB Extension Functions \u00b6 Name Summary transactionWritingPager fun <DB : LogicalDb , T> DB .transactionWritingPager(items: List < T >, maxTransactionItems: Int = 25, handler: WritingPager.Handler < T >): WritingPager < T >","title":"LogicalDb - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/#logicaldb","text":"interface LogicalDb : LogicalTable.Factory A collection of tables that implement the DynamoDB best practice of putting multiple item types into the same storage table. This makes it possible to perform aggregate operations and transactions on those item types.","title":"LogicalDb"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/#functions","text":"Name Summary batchLoad abstract fun batchLoad(keys: KeySet , consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet Retrieves multiple items from multiple tables using their primary keys. open fun batchLoad(keys: Iterable < Any >, consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet open fun batchLoad(vararg keys: Any , consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet batchWrite abstract fun batchWrite(writeSet: BatchWriteSet , retryStrategy: BatchWriteRetryStrategy = DefaultBatchWriteRetryStrategy()): BatchWriteResult Saves and deletes the objects given using one or more calls to the AmazonDynamoDB.batchWriteItem API. Callers should always check the returned BatchWriteResult because this method returns normally even if some writes were not performed. transactionLoad abstract fun transactionLoad(keys: KeySet ): ItemSet Transactionally loads objects specified by transactionLoadRequest by calling AmazonDynamoDB.transactGetItems API. open fun transactionLoad(keys: Iterable < Any >): ItemSet open fun transactionLoad(vararg keys: Any ): ItemSet transactionWrite abstract fun transactionWrite(writeSet: TransactionWriteSet ): Unit Transactionally writes objects specified by transactionWriteRequest by calling AmazonDynamoDB.transactWriteItems API.","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/#inherited-functions","text":"Name Summary logicalTable abstract fun <T : LogicalTable < RI >, RI : Any > logicalTable(tableType: KClass < T >): T","title":"Inherited Functions"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/#companion-object-functions","text":"Name Summary create fun <DB : LogicalDb > create(dbType: KClass < DB >, dynamoDbMapper: DynamoDBMapper , config: DynamoDBMapperConfig = DynamoDBMapperConfig.DEFAULT): DB invoke operator fun <DB : LogicalDb > invoke(dynamoDbMapper: DynamoDBMapper , config: DynamoDBMapperConfig = DynamoDBMapperConfig.DEFAULT): DB","title":"Companion Object Functions"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/#extension-functions","text":"Name Summary transactionWritingPager fun <DB : LogicalDb , T> DB .transactionWritingPager(items: List < T >, maxTransactionItems: Int = 25, handler: WritingPager.Handler < T >): WritingPager < T >","title":"Extension Functions"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/batch-load/","text":"tempest / app.cash.tempest / LogicalDb / batchLoad batchLoad \u00b6 abstract fun batchLoad(keys: KeySet , consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet Retrieves multiple items from multiple tables using their primary keys. This method performs one or more calls to the AmazonDynamoDB.batchGetItem API. A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. BatchGetItem returns a partial result if the response size limit is exceeded, the table\u2019s provisioned throughput is exceeded, or an internal processing failure occurs. If a partial result is returned, this method backs off and retries the UnprocessedKeys in the next API call. open fun batchLoad(keys: Iterable < Any >, consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet open fun batchLoad(vararg keys: Any , consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet","title":"batchLoad - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/batch-load/#batchload","text":"abstract fun batchLoad(keys: KeySet , consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet Retrieves multiple items from multiple tables using their primary keys. This method performs one or more calls to the AmazonDynamoDB.batchGetItem API. A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. BatchGetItem returns a partial result if the response size limit is exceeded, the table\u2019s provisioned throughput is exceeded, or an internal processing failure occurs. If a partial result is returned, this method backs off and retries the UnprocessedKeys in the next API call. open fun batchLoad(keys: Iterable < Any >, consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet open fun batchLoad(vararg keys: Any , consistentReads: ConsistentReads = EVENTUAL, retryStrategy: BatchLoadRetryStrategy = DefaultBatchLoadRetryStrategy()): ItemSet","title":"batchLoad"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/batch-write/","text":"tempest / app.cash.tempest / LogicalDb / batchWrite batchWrite \u00b6 @CheckReturnValue abstract fun batchWrite(writeSet: BatchWriteSet , retryStrategy: BatchWriteRetryStrategy = DefaultBatchWriteRetryStrategy()): BatchWriteResult Saves and deletes the objects given using one or more calls to the AmazonDynamoDB.batchWriteItem API. Callers should always check the returned BatchWriteResult because this method returns normally even if some writes were not performed. This method does not support versioning annotations and behaves as if DynamoDBMapperConfig.SaveBehavior.CLOBBER was specified. A single call to BatchWriteItem can write up to 16 MB of data, which can comprise as many as 25 put or delete requests. Individual items to be written can be as large as 400 KB. In order to improve performance with these large-scale operations, this does not behave in the same way as individual PutItem and DeleteItem calls would. For example, you cannot specify conditions on individual put and delete requests, and BatchWriteItem does not return deleted items in the response.","title":"batchWrite - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/batch-write/#batchwrite","text":"@CheckReturnValue abstract fun batchWrite(writeSet: BatchWriteSet , retryStrategy: BatchWriteRetryStrategy = DefaultBatchWriteRetryStrategy()): BatchWriteResult Saves and deletes the objects given using one or more calls to the AmazonDynamoDB.batchWriteItem API. Callers should always check the returned BatchWriteResult because this method returns normally even if some writes were not performed. This method does not support versioning annotations and behaves as if DynamoDBMapperConfig.SaveBehavior.CLOBBER was specified. A single call to BatchWriteItem can write up to 16 MB of data, which can comprise as many as 25 put or delete requests. Individual items to be written can be as large as 400 KB. In order to improve performance with these large-scale operations, this does not behave in the same way as individual PutItem and DeleteItem calls would. For example, you cannot specify conditions on individual put and delete requests, and BatchWriteItem does not return deleted items in the response.","title":"batchWrite"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/create/","text":"tempest / app.cash.tempest / LogicalDb / create create \u00b6 fun <DB : LogicalDb > create(dbType: KClass < DB >, dynamoDbMapper: DynamoDBMapper , config: DynamoDBMapperConfig = DynamoDBMapperConfig.DEFAULT): DB","title":"create - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/create/#create","text":"fun <DB : LogicalDb > create(dbType: KClass < DB >, dynamoDbMapper: DynamoDBMapper , config: DynamoDBMapperConfig = DynamoDBMapperConfig.DEFAULT): DB","title":"create"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/invoke/","text":"tempest / app.cash.tempest / LogicalDb / invoke invoke \u00b6 inline operator fun <reified DB : LogicalDb > invoke(dynamoDbMapper: DynamoDBMapper , config: DynamoDBMapperConfig = DynamoDBMapperConfig.DEFAULT): DB","title":"invoke - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/invoke/#invoke","text":"inline operator fun <reified DB : LogicalDb > invoke(dynamoDbMapper: DynamoDBMapper , config: DynamoDBMapperConfig = DynamoDBMapperConfig.DEFAULT): DB","title":"invoke"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/transaction-load/","text":"tempest / app.cash.tempest / LogicalDb / transactionLoad transactionLoad \u00b6 abstract fun transactionLoad(keys: KeySet ): ItemSet Transactionally loads objects specified by transactionLoadRequest by calling AmazonDynamoDB.transactGetItems API. A transaction cannot contain more than 25 unique items. A transaction cannot contain more than 4 MB of data. No two actions in a transaction can work against the same item in the same table. open fun transactionLoad(keys: Iterable < Any >): ItemSet open fun transactionLoad(vararg keys: Any ): ItemSet","title":"transactionLoad - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/transaction-load/#transactionload","text":"abstract fun transactionLoad(keys: KeySet ): ItemSet Transactionally loads objects specified by transactionLoadRequest by calling AmazonDynamoDB.transactGetItems API. A transaction cannot contain more than 25 unique items. A transaction cannot contain more than 4 MB of data. No two actions in a transaction can work against the same item in the same table. open fun transactionLoad(keys: Iterable < Any >): ItemSet open fun transactionLoad(vararg keys: Any ): ItemSet","title":"transactionLoad"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/transaction-write/","text":"tempest / app.cash.tempest / LogicalDb / transactionWrite transactionWrite \u00b6 abstract fun transactionWrite(writeSet: TransactionWriteSet ): Unit Transactionally writes objects specified by transactionWriteRequest by calling AmazonDynamoDB.transactWriteItems API. This method does not support versioning annotations. It throws com.amazonaws.SdkClientException exception if class of any input object is annotated with DynamoDBVersionAttribute or DynamoDBVersioned . A transaction cannot contain more than 25 unique items, including conditions. A transaction cannot contain more than 4 MB of data. No two actions in a transaction can work against the same item in the same table. For example, you cannot both ConditionCheck and Update the same item in one transaction.","title":"transactionWrite - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-logical-db/transaction-write/#transactionwrite","text":"abstract fun transactionWrite(writeSet: TransactionWriteSet ): Unit Transactionally writes objects specified by transactionWriteRequest by calling AmazonDynamoDB.transactWriteItems API. This method does not support versioning annotations. It throws com.amazonaws.SdkClientException exception if class of any input object is annotated with DynamoDBVersionAttribute or DynamoDBVersioned . A transaction cannot contain more than 25 unique items, including conditions. A transaction cannot contain more than 4 MB of data. No two actions in a transaction can work against the same item in the same table. For example, you cannot both ConditionCheck and Update the same item in one transaction.","title":"transactionWrite"},{"location":"0.x/tempest/app.cash.tempest/-logical-table/","text":"tempest / app.cash.tempest / LogicalTable LogicalTable \u00b6 interface LogicalTable<RI : Any > : View < RI , RI >, InlineView.Factory , SecondaryIndex.Factory A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes. Types \u00b6 Name Summary Factory interface Factory Inherited Functions \u00b6 Name Summary delete abstract fun delete(item: I , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes item from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. deleteKey abstract fun deleteKey(key: K , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes the item identified by key from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. inlineView abstract fun <K : Any , I : Any > inlineView(keyType: KClass < K >, itemType: KClass < I >): InlineView < K , I > load abstract fun load(key: K , consistentReads: ConsistentReads = ConsistentReads.EVENTUAL): I ? Returns an item whose keys match those of the prototype key object given, or null if no such item exists. save abstract fun save(item: I , saveExpression: DynamoDBSaveExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Saves an item in DynamoDB. This method uses DynamoDBMapperConfig.SaveBehavior.PUT to clear and replace all attributes, including unmodeled ones, on save. Partial update, i.e. DynamoDBMapperConfig.SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES , is not supported yet. secondaryIndex abstract fun <K : Any , I : Any > secondaryIndex(keyType: KClass < K >, itemType: KClass < I >): SecondaryIndex < K , I >","title":"LogicalTable - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-logical-table/#logicaltable","text":"interface LogicalTable<RI : Any > : View < RI , RI >, InlineView.Factory , SecondaryIndex.Factory A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes.","title":"LogicalTable"},{"location":"0.x/tempest/app.cash.tempest/-logical-table/#types","text":"Name Summary Factory interface Factory","title":"Types"},{"location":"0.x/tempest/app.cash.tempest/-logical-table/#inherited-functions","text":"Name Summary delete abstract fun delete(item: I , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes item from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. deleteKey abstract fun deleteKey(key: K , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes the item identified by key from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. inlineView abstract fun <K : Any , I : Any > inlineView(keyType: KClass < K >, itemType: KClass < I >): InlineView < K , I > load abstract fun load(key: K , consistentReads: ConsistentReads = ConsistentReads.EVENTUAL): I ? Returns an item whose keys match those of the prototype key object given, or null if no such item exists. save abstract fun save(item: I , saveExpression: DynamoDBSaveExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Saves an item in DynamoDB. This method uses DynamoDBMapperConfig.SaveBehavior.PUT to clear and replace all attributes, including unmodeled ones, on save. Partial update, i.e. DynamoDBMapperConfig.SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES , is not supported yet. secondaryIndex abstract fun <K : Any , I : Any > secondaryIndex(keyType: KClass < K >, itemType: KClass < I >): SecondaryIndex < K , I >","title":"Inherited Functions"},{"location":"0.x/tempest/app.cash.tempest/-logical-table/-factory/","text":"tempest / app.cash.tempest / LogicalTable / Factory Factory \u00b6 interface Factory Functions \u00b6 Name Summary logicalTable abstract fun <T : LogicalTable < RI >, RI : Any > logicalTable(tableType: KClass < T >): T Inheritors \u00b6 Name Summary LogicalDb interface LogicalDb : LogicalTable.Factory A collection of tables that implement the DynamoDB best practice of putting multiple item types into the same storage table. This makes it possible to perform aggregate operations and transactions on those item types.","title":"Factory - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-logical-table/-factory/#factory","text":"interface Factory","title":"Factory"},{"location":"0.x/tempest/app.cash.tempest/-logical-table/-factory/#functions","text":"Name Summary logicalTable abstract fun <T : LogicalTable < RI >, RI : Any > logicalTable(tableType: KClass < T >): T","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-logical-table/-factory/#inheritors","text":"Name Summary LogicalDb interface LogicalDb : LogicalTable.Factory A collection of tables that implement the DynamoDB best practice of putting multiple item types into the same storage table. This makes it possible to perform aggregate operations and transactions on those item types.","title":"Inheritors"},{"location":"0.x/tempest/app.cash.tempest/-logical-table/-factory/logical-table/","text":"tempest / app.cash.tempest / LogicalTable / Factory / logicalTable logicalTable \u00b6 abstract fun <T : LogicalTable < RI >, RI : Any > logicalTable(tableType: KClass < T >): T","title":"logicalTable - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-logical-table/-factory/logical-table/#logicaltable","text":"abstract fun <T : LogicalTable < RI >, RI : Any > logicalTable(tableType: KClass < T >): T","title":"logicalTable"},{"location":"0.x/tempest/app.cash.tempest/-offset/","text":"tempest / app.cash.tempest / Offset Offset \u00b6 data class Offset<K> Constructors \u00b6 Name Summary <init> Offset(key: K ) Properties \u00b6 Name Summary key val key: K","title":"Offset - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-offset/#offset","text":"data class Offset<K>","title":"Offset"},{"location":"0.x/tempest/app.cash.tempest/-offset/#constructors","text":"Name Summary <init> Offset(key: K )","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-offset/#properties","text":"Name Summary key val key: K","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-offset/-init-/","text":"tempest / app.cash.tempest / Offset / <init> <init> \u00b6 Offset(key: K )","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-offset/-init-/#init","text":"Offset(key: K )","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-offset/key/","text":"tempest / app.cash.tempest / Offset / key key \u00b6 val key: K","title":"key - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-offset/key/#key","text":"val key: K","title":"key"},{"location":"0.x/tempest/app.cash.tempest/-page/","text":"tempest / app.cash.tempest / Page Page \u00b6 data class Page<K, T> Constructors \u00b6 Name Summary <init> Page(contents: List < T >, offset: Offset < K >?, scannedCount: Int , consumedCapacity: ConsumedCapacity ?) Properties \u00b6 Name Summary consumedCapacity val consumedCapacity: ConsumedCapacity ? The data returned includes the total provisioned throughput consumed, along with statistics for the table and any indexes involved in the operation. This is only returned if the ReturnConsumedCapacity parameter was specified. contents val contents: List < T > hasMorePages val hasMorePages: Boolean offset val offset: Offset < K >? scannedCount val scannedCount: Int The number of items evaluated, before any filter is applied.","title":"Page - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-page/#page","text":"data class Page<K, T>","title":"Page"},{"location":"0.x/tempest/app.cash.tempest/-page/#constructors","text":"Name Summary <init> Page(contents: List < T >, offset: Offset < K >?, scannedCount: Int , consumedCapacity: ConsumedCapacity ?)","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-page/#properties","text":"Name Summary consumedCapacity val consumedCapacity: ConsumedCapacity ? The data returned includes the total provisioned throughput consumed, along with statistics for the table and any indexes involved in the operation. This is only returned if the ReturnConsumedCapacity parameter was specified. contents val contents: List < T > hasMorePages val hasMorePages: Boolean offset val offset: Offset < K >? scannedCount val scannedCount: Int The number of items evaluated, before any filter is applied.","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-page/-init-/","text":"tempest / app.cash.tempest / Page / <init> <init> \u00b6 Page(contents: List < T >, offset: Offset < K >?, scannedCount: Int , consumedCapacity: ConsumedCapacity ?)","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-page/-init-/#init","text":"Page(contents: List < T >, offset: Offset < K >?, scannedCount: Int , consumedCapacity: ConsumedCapacity ?)","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-page/consumed-capacity/","text":"tempest / app.cash.tempest / Page / consumedCapacity consumedCapacity \u00b6 val consumedCapacity: ConsumedCapacity ? The data returned includes the total provisioned throughput consumed, along with statistics for the table and any indexes involved in the operation. This is only returned if the ReturnConsumedCapacity parameter was specified.","title":"consumedCapacity - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-page/consumed-capacity/#consumedcapacity","text":"val consumedCapacity: ConsumedCapacity ? The data returned includes the total provisioned throughput consumed, along with statistics for the table and any indexes involved in the operation. This is only returned if the ReturnConsumedCapacity parameter was specified.","title":"consumedCapacity"},{"location":"0.x/tempest/app.cash.tempest/-page/contents/","text":"tempest / app.cash.tempest / Page / contents contents \u00b6 val contents: List < T >","title":"contents - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-page/contents/#contents","text":"val contents: List < T >","title":"contents"},{"location":"0.x/tempest/app.cash.tempest/-page/has-more-pages/","text":"tempest / app.cash.tempest / Page / hasMorePages hasMorePages \u00b6 val hasMorePages: Boolean","title":"hasMorePages - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-page/has-more-pages/#hasmorepages","text":"val hasMorePages: Boolean","title":"hasMorePages"},{"location":"0.x/tempest/app.cash.tempest/-page/offset/","text":"tempest / app.cash.tempest / Page / offset offset \u00b6 val offset: Offset < K >?","title":"offset - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-page/offset/#offset","text":"val offset: Offset < K >?","title":"offset"},{"location":"0.x/tempest/app.cash.tempest/-page/scanned-count/","text":"tempest / app.cash.tempest / Page / scannedCount scannedCount \u00b6 val scannedCount: Int The number of items evaluated, before any filter is applied.","title":"scannedCount - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-page/scanned-count/#scannedcount","text":"val scannedCount: Int The number of items evaluated, before any filter is applied.","title":"scannedCount"},{"location":"0.x/tempest/app.cash.tempest/-queryable/","text":"tempest / app.cash.tempest / Queryable Queryable \u00b6 interface Queryable<K : Any , I : Any > Functions \u00b6 Name Summary query abstract fun query(keyCondition: KeyCondition < K >, consistentRead: Boolean = false, asc: Boolean = true, pageSize: Int = 100, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Reads up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated. Inheritors \u00b6 Name Summary InlineView interface InlineView<K : Any , I : Any > : View < K , I >, Scannable < K , I >, Queryable < K , I > SecondaryIndex interface SecondaryIndex<K : Any , I : Any > : Scannable < K , I >, Queryable < K , I >","title":"Queryable - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-queryable/#queryable","text":"interface Queryable<K : Any , I : Any >","title":"Queryable"},{"location":"0.x/tempest/app.cash.tempest/-queryable/#functions","text":"Name Summary query abstract fun query(keyCondition: KeyCondition < K >, consistentRead: Boolean = false, asc: Boolean = true, pageSize: Int = 100, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Reads up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-queryable/#inheritors","text":"Name Summary InlineView interface InlineView<K : Any , I : Any > : View < K , I >, Scannable < K , I >, Queryable < K , I > SecondaryIndex interface SecondaryIndex<K : Any , I : Any > : Scannable < K , I >, Queryable < K , I >","title":"Inheritors"},{"location":"0.x/tempest/app.cash.tempest/-queryable/query/","text":"tempest / app.cash.tempest / Queryable / query query \u00b6 abstract fun query(keyCondition: KeyCondition < K >, consistentRead: Boolean = false, asc: Boolean = true, pageSize: Int = 100, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Reads up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.","title":"query - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-queryable/query/#query","text":"abstract fun query(keyCondition: KeyCondition < K >, consistentRead: Boolean = false, asc: Boolean = true, pageSize: Int = 100, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Reads up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.","title":"query"},{"location":"0.x/tempest/app.cash.tempest/-scannable/","text":"tempest / app.cash.tempest / Scannable Scannable \u00b6 interface Scannable<K : Any , I : Any > Functions \u00b6 Name Summary scan abstract fun scan(workerId: WorkerId = WorkerId.SEQUENTIAL, pageSize: Int = 100, consistentRead: Boolean = false, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Scans up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated. Inheritors \u00b6 Name Summary InlineView interface InlineView<K : Any , I : Any > : View < K , I >, Scannable < K , I >, Queryable < K , I > SecondaryIndex interface SecondaryIndex<K : Any , I : Any > : Scannable < K , I >, Queryable < K , I >","title":"Scannable - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-scannable/#scannable","text":"interface Scannable<K : Any , I : Any >","title":"Scannable"},{"location":"0.x/tempest/app.cash.tempest/-scannable/#functions","text":"Name Summary scan abstract fun scan(workerId: WorkerId = WorkerId.SEQUENTIAL, pageSize: Int = 100, consistentRead: Boolean = false, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Scans up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-scannable/#inheritors","text":"Name Summary InlineView interface InlineView<K : Any , I : Any > : View < K , I >, Scannable < K , I >, Queryable < K , I > SecondaryIndex interface SecondaryIndex<K : Any , I : Any > : Scannable < K , I >, Queryable < K , I >","title":"Inheritors"},{"location":"0.x/tempest/app.cash.tempest/-scannable/scan/","text":"tempest / app.cash.tempest / Scannable / scan scan \u00b6 abstract fun scan(workerId: WorkerId = WorkerId.SEQUENTIAL, pageSize: Int = 100, consistentRead: Boolean = false, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Scans up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.","title":"scan - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-scannable/scan/#scan","text":"abstract fun scan(workerId: WorkerId = WorkerId.SEQUENTIAL, pageSize: Int = 100, consistentRead: Boolean = false, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Scans up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.","title":"scan"},{"location":"0.x/tempest/app.cash.tempest/-secondary-index/","text":"tempest / app.cash.tempest / SecondaryIndex SecondaryIndex \u00b6 interface SecondaryIndex<K : Any , I : Any > : Scannable < K , I >, Queryable < K , I > Types \u00b6 Name Summary Factory interface Factory Inherited Functions \u00b6 Name Summary query abstract fun query(keyCondition: KeyCondition < K >, consistentRead: Boolean = false, asc: Boolean = true, pageSize: Int = 100, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Reads up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated. scan abstract fun scan(workerId: WorkerId = WorkerId.SEQUENTIAL, pageSize: Int = 100, consistentRead: Boolean = false, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Scans up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.","title":"SecondaryIndex - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-secondary-index/#secondaryindex","text":"interface SecondaryIndex<K : Any , I : Any > : Scannable < K , I >, Queryable < K , I >","title":"SecondaryIndex"},{"location":"0.x/tempest/app.cash.tempest/-secondary-index/#types","text":"Name Summary Factory interface Factory","title":"Types"},{"location":"0.x/tempest/app.cash.tempest/-secondary-index/#inherited-functions","text":"Name Summary query abstract fun query(keyCondition: KeyCondition < K >, consistentRead: Boolean = false, asc: Boolean = true, pageSize: Int = 100, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Reads up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated. scan abstract fun scan(workerId: WorkerId = WorkerId.SEQUENTIAL, pageSize: Int = 100, consistentRead: Boolean = false, returnConsumedCapacity: ReturnConsumedCapacity = NONE, filterExpression: FilterExpression ? = null, initialOffset: Offset < K >? = null): Page < K , I > Scans up to the pageSize items or a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.","title":"Inherited Functions"},{"location":"0.x/tempest/app.cash.tempest/-secondary-index/-factory/","text":"tempest / app.cash.tempest / SecondaryIndex / Factory Factory \u00b6 interface Factory Functions \u00b6 Name Summary secondaryIndex abstract fun <K : Any , I : Any > secondaryIndex(keyType: KClass < K >, itemType: KClass < I >): SecondaryIndex < K , I > Inheritors \u00b6 Name Summary LogicalTable interface LogicalTable<RI : Any > : View < RI , RI >, InlineView.Factory , SecondaryIndex.Factory A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes.","title":"Factory - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-secondary-index/-factory/#factory","text":"interface Factory","title":"Factory"},{"location":"0.x/tempest/app.cash.tempest/-secondary-index/-factory/#functions","text":"Name Summary secondaryIndex abstract fun <K : Any , I : Any > secondaryIndex(keyType: KClass < K >, itemType: KClass < I >): SecondaryIndex < K , I >","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-secondary-index/-factory/#inheritors","text":"Name Summary LogicalTable interface LogicalTable<RI : Any > : View < RI , RI >, InlineView.Factory , SecondaryIndex.Factory A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes.","title":"Inheritors"},{"location":"0.x/tempest/app.cash.tempest/-secondary-index/-factory/secondary-index/","text":"tempest / app.cash.tempest / SecondaryIndex / Factory / secondaryIndex secondaryIndex \u00b6 abstract fun <K : Any , I : Any > secondaryIndex(keyType: KClass < K >, itemType: KClass < I >): SecondaryIndex < K , I >","title":"secondaryIndex - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-secondary-index/-factory/secondary-index/#secondaryindex","text":"abstract fun <K : Any , I : Any > secondaryIndex(keyType: KClass < K >, itemType: KClass < I >): SecondaryIndex < K , I >","title":"secondaryIndex"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/","text":"tempest / app.cash.tempest / TransactionWriteSet TransactionWriteSet \u00b6 data class TransactionWriteSet Types \u00b6 Name Summary Builder class Builder Constructors \u00b6 Name Summary <init> TransactionWriteSet(itemsToSave: ItemSet , keysToDelete: KeySet , keysToCheck: KeySet , writeExpressions: Map < Any , DynamoDBTransactionWriteExpression >, idempotencyToken: String ?) Properties \u00b6 Name Summary idempotencyToken val idempotencyToken: String ? itemsToSave val itemsToSave: ItemSet keysToCheck val keysToCheck: KeySet keysToDelete val keysToDelete: KeySet size val size: Int writeExpressions val writeExpressions: Map < Any , DynamoDBTransactionWriteExpression >","title":"TransactionWriteSet - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/#transactionwriteset","text":"data class TransactionWriteSet","title":"TransactionWriteSet"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/#types","text":"Name Summary Builder class Builder","title":"Types"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/#constructors","text":"Name Summary <init> TransactionWriteSet(itemsToSave: ItemSet , keysToDelete: KeySet , keysToCheck: KeySet , writeExpressions: Map < Any , DynamoDBTransactionWriteExpression >, idempotencyToken: String ?)","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/#properties","text":"Name Summary idempotencyToken val idempotencyToken: String ? itemsToSave val itemsToSave: ItemSet keysToCheck val keysToCheck: KeySet keysToDelete val keysToDelete: KeySet size val size: Int writeExpressions val writeExpressions: Map < Any , DynamoDBTransactionWriteExpression >","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-init-/","text":"tempest / app.cash.tempest / TransactionWriteSet / <init> <init> \u00b6 TransactionWriteSet(itemsToSave: ItemSet , keysToDelete: KeySet , keysToCheck: KeySet , writeExpressions: Map < Any , DynamoDBTransactionWriteExpression >, idempotencyToken: String ?)","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-init-/#init","text":"TransactionWriteSet(itemsToSave: ItemSet , keysToDelete: KeySet , keysToCheck: KeySet , writeExpressions: Map < Any , DynamoDBTransactionWriteExpression >, idempotencyToken: String ?)","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/idempotency-token/","text":"tempest / app.cash.tempest / TransactionWriteSet / idempotencyToken idempotencyToken \u00b6 val idempotencyToken: String ?","title":"idempotencyToken - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/idempotency-token/#idempotencytoken","text":"val idempotencyToken: String ?","title":"idempotencyToken"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/items-to-save/","text":"tempest / app.cash.tempest / TransactionWriteSet / itemsToSave itemsToSave \u00b6 val itemsToSave: ItemSet","title":"itemsToSave - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/items-to-save/#itemstosave","text":"val itemsToSave: ItemSet","title":"itemsToSave"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/keys-to-check/","text":"tempest / app.cash.tempest / TransactionWriteSet / keysToCheck keysToCheck \u00b6 val keysToCheck: KeySet","title":"keysToCheck - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/keys-to-check/#keystocheck","text":"val keysToCheck: KeySet","title":"keysToCheck"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/keys-to-delete/","text":"tempest / app.cash.tempest / TransactionWriteSet / keysToDelete keysToDelete \u00b6 val keysToDelete: KeySet","title":"keysToDelete - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/keys-to-delete/#keystodelete","text":"val keysToDelete: KeySet","title":"keysToDelete"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/size/","text":"tempest / app.cash.tempest / TransactionWriteSet / size size \u00b6 val size: Int","title":"size - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/size/#size","text":"val size: Int","title":"size"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/write-expressions/","text":"tempest / app.cash.tempest / TransactionWriteSet / writeExpressions writeExpressions \u00b6 val writeExpressions: Map < Any , DynamoDBTransactionWriteExpression >","title":"writeExpressions - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/write-expressions/#writeexpressions","text":"val writeExpressions: Map < Any , DynamoDBTransactionWriteExpression >","title":"writeExpressions"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/","text":"tempest / app.cash.tempest / TransactionWriteSet / Builder Builder \u00b6 class Builder Constructors \u00b6 Name Summary <init> Builder() Properties \u00b6 Name Summary size val size: Int Functions \u00b6 Name Summary addAll fun addAll(builder: TransactionWriteSet.Builder ): Unit build fun build(): TransactionWriteSet checkCondition fun checkCondition(key: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder delete fun delete(key: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder idempotencyToken fun idempotencyToken(idempotencyToken: String ): TransactionWriteSet.Builder save fun save(item: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder This adds a put operation to clear and replace all attributes, including unmodeled ones. Partial update is not supported.","title":"Builder - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/#builder","text":"class Builder","title":"Builder"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/#constructors","text":"Name Summary <init> Builder()","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/#properties","text":"Name Summary size val size: Int","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/#functions","text":"Name Summary addAll fun addAll(builder: TransactionWriteSet.Builder ): Unit build fun build(): TransactionWriteSet checkCondition fun checkCondition(key: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder delete fun delete(key: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder idempotencyToken fun idempotencyToken(idempotencyToken: String ): TransactionWriteSet.Builder save fun save(item: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder This adds a put operation to clear and replace all attributes, including unmodeled ones. Partial update is not supported.","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/-init-/","text":"tempest / app.cash.tempest / TransactionWriteSet / Builder / <init> <init> \u00b6 Builder()","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/-init-/#init","text":"Builder()","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/add-all/","text":"tempest / app.cash.tempest / TransactionWriteSet / Builder / addAll addAll \u00b6 fun addAll(builder: TransactionWriteSet.Builder ): Unit","title":"addAll - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/add-all/#addall","text":"fun addAll(builder: TransactionWriteSet.Builder ): Unit","title":"addAll"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/build/","text":"tempest / app.cash.tempest / TransactionWriteSet / Builder / build build \u00b6 fun build(): TransactionWriteSet","title":"build - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/build/#build","text":"fun build(): TransactionWriteSet","title":"build"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/check-condition/","text":"tempest / app.cash.tempest / TransactionWriteSet / Builder / checkCondition checkCondition \u00b6 fun checkCondition(key: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder","title":"checkCondition - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/check-condition/#checkcondition","text":"fun checkCondition(key: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder","title":"checkCondition"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/delete/","text":"tempest / app.cash.tempest / TransactionWriteSet / Builder / delete delete \u00b6 fun delete(key: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder","title":"delete - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/delete/#delete","text":"fun delete(key: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder","title":"delete"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/idempotency-token/","text":"tempest / app.cash.tempest / TransactionWriteSet / Builder / idempotencyToken idempotencyToken \u00b6 fun idempotencyToken(idempotencyToken: String ): TransactionWriteSet.Builder","title":"idempotencyToken - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/idempotency-token/#idempotencytoken","text":"fun idempotencyToken(idempotencyToken: String ): TransactionWriteSet.Builder","title":"idempotencyToken"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/save/","text":"tempest / app.cash.tempest / TransactionWriteSet / Builder / save save \u00b6 fun save(item: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder This adds a put operation to clear and replace all attributes, including unmodeled ones. Partial update is not supported.","title":"save - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/save/#save","text":"fun save(item: Any , expression: DynamoDBTransactionWriteExpression ? = null): TransactionWriteSet.Builder This adds a put operation to clear and replace all attributes, including unmodeled ones. Partial update is not supported.","title":"save"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/size/","text":"tempest / app.cash.tempest / TransactionWriteSet / Builder / size size \u00b6 val size: Int","title":"size - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-transaction-write-set/-builder/size/#size","text":"val size: Int","title":"size"},{"location":"0.x/tempest/app.cash.tempest/-view/","text":"tempest / app.cash.tempest / View View \u00b6 interface View<K : Any , I : Any > Functions \u00b6 Name Summary delete abstract fun delete(item: I , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes item from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. deleteKey abstract fun deleteKey(key: K , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes the item identified by key from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. load abstract fun load(key: K , consistentReads: ConsistentReads = ConsistentReads.EVENTUAL): I ? Returns an item whose keys match those of the prototype key object given, or null if no such item exists. save abstract fun save(item: I , saveExpression: DynamoDBSaveExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Saves an item in DynamoDB. This method uses DynamoDBMapperConfig.SaveBehavior.PUT to clear and replace all attributes, including unmodeled ones, on save. Partial update, i.e. DynamoDBMapperConfig.SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES , is not supported yet. Inheritors \u00b6 Name Summary InlineView interface InlineView<K : Any , I : Any > : View < K , I >, Scannable < K , I >, Queryable < K , I > LogicalTable interface LogicalTable<RI : Any > : View < RI , RI >, InlineView.Factory , SecondaryIndex.Factory A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes.","title":"View - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-view/#view","text":"interface View<K : Any , I : Any >","title":"View"},{"location":"0.x/tempest/app.cash.tempest/-view/#functions","text":"Name Summary delete abstract fun delete(item: I , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes item from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. deleteKey abstract fun deleteKey(key: K , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes the item identified by key from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. load abstract fun load(key: K , consistentReads: ConsistentReads = ConsistentReads.EVENTUAL): I ? Returns an item whose keys match those of the prototype key object given, or null if no such item exists. save abstract fun save(item: I , saveExpression: DynamoDBSaveExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Saves an item in DynamoDB. This method uses DynamoDBMapperConfig.SaveBehavior.PUT to clear and replace all attributes, including unmodeled ones, on save. Partial update, i.e. DynamoDBMapperConfig.SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES , is not supported yet.","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-view/#inheritors","text":"Name Summary InlineView interface InlineView<K : Any , I : Any > : View < K , I >, Scannable < K , I >, Queryable < K , I > LogicalTable interface LogicalTable<RI : Any > : View < RI , RI >, InlineView.Factory , SecondaryIndex.Factory A collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes.","title":"Inheritors"},{"location":"0.x/tempest/app.cash.tempest/-view/delete-key/","text":"tempest / app.cash.tempest / View / deleteKey deleteKey \u00b6 abstract fun deleteKey(key: K , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes the item identified by key from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. If the item to be deleted has versioned attributes, load the item and use delete instead or use ignoreVersionConstraints to discard them.","title":"deleteKey - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-view/delete-key/#deletekey","text":"abstract fun deleteKey(key: K , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes the item identified by key from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. If the item to be deleted has versioned attributes, load the item and use delete instead or use ignoreVersionConstraints to discard them.","title":"deleteKey"},{"location":"0.x/tempest/app.cash.tempest/-view/delete/","text":"tempest / app.cash.tempest / View / delete delete \u00b6 abstract fun delete(item: I , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes item from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. If ignoreVersionConstraints is true, version attributes will not be considered when deleting the object.","title":"delete - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-view/delete/#delete","text":"abstract fun delete(item: I , deleteExpression: DynamoDBDeleteExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Deletes item from its DynamoDB table using deleteExpression . Any options specified in the deleteExpression parameter will be overlaid on any constraints due to versioned attributes. If ignoreVersionConstraints is true, version attributes will not be considered when deleting the object.","title":"delete"},{"location":"0.x/tempest/app.cash.tempest/-view/load/","text":"tempest / app.cash.tempest / View / load load \u00b6 abstract fun load(key: K , consistentReads: ConsistentReads = ConsistentReads.EVENTUAL): I ? Returns an item whose keys match those of the prototype key object given, or null if no such item exists.","title":"load - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-view/load/#load","text":"abstract fun load(key: K , consistentReads: ConsistentReads = ConsistentReads.EVENTUAL): I ? Returns an item whose keys match those of the prototype key object given, or null if no such item exists.","title":"load"},{"location":"0.x/tempest/app.cash.tempest/-view/save/","text":"tempest / app.cash.tempest / View / save save \u00b6 abstract fun save(item: I , saveExpression: DynamoDBSaveExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Saves an item in DynamoDB. This method uses DynamoDBMapperConfig.SaveBehavior.PUT to clear and replace all attributes, including unmodeled ones, on save. Partial update, i.e. DynamoDBMapperConfig.SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES , is not supported yet. Any options specified in the saveExpression parameter will be overlaid on any constraints due to versioned attributes. If ignoreVersionConstraints is true, version attributes will be discarded.","title":"save - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-view/save/#save","text":"abstract fun save(item: I , saveExpression: DynamoDBSaveExpression ? = null, ignoreVersionConstraints: Boolean = false): Unit Saves an item in DynamoDB. This method uses DynamoDBMapperConfig.SaveBehavior.PUT to clear and replace all attributes, including unmodeled ones, on save. Partial update, i.e. DynamoDBMapperConfig.SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES , is not supported yet. Any options specified in the saveExpression parameter will be overlaid on any constraints due to versioned attributes. If ignoreVersionConstraints is true, version attributes will be discarded.","title":"save"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/","text":"tempest / app.cash.tempest / WorkerId WorkerId \u00b6 data class WorkerId By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table\u2019s data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with an unique WorkerId . Constructors \u00b6 Name Summary <init> WorkerId(segment: Int = 0, totalSegments: Int = 1) By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. Properties \u00b6 Name Summary segment val segment: Int A segment to be scanned by a particular worker. Each worker should use a different value for Segment. totalSegments val totalSegments: Int The total number of segments for the parallel scan. This value must be the same as the number of workers that your application will use. Companion Object Properties \u00b6 Name Summary SEQUENTIAL val SEQUENTIAL: WorkerId","title":"WorkerId - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/#workerid","text":"data class WorkerId By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table\u2019s data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with an unique WorkerId .","title":"WorkerId"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/#constructors","text":"Name Summary <init> WorkerId(segment: Int = 0, totalSegments: Int = 1) By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data.","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/#properties","text":"Name Summary segment val segment: Int A segment to be scanned by a particular worker. Each worker should use a different value for Segment. totalSegments val totalSegments: Int The total number of segments for the parallel scan. This value must be the same as the number of workers that your application will use.","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/#companion-object-properties","text":"Name Summary SEQUENTIAL val SEQUENTIAL: WorkerId","title":"Companion Object Properties"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/-init-/","text":"tempest / app.cash.tempest / WorkerId / <init> <init> \u00b6 WorkerId(segment: Int = 0, totalSegments: Int = 1) By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table\u2019s data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with an unique WorkerId .","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-worker-id/-init-/#init","text":"WorkerId(segment: Int = 0, totalSegments: Int = 1) By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table\u2019s data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with an unique WorkerId .","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/-s-e-q-u-e-n-t-i-a-l/","text":"tempest / app.cash.tempest / WorkerId / SEQUENTIAL SEQUENTIAL \u00b6 val SEQUENTIAL: WorkerId","title":"SEQUENTIAL - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/-s-e-q-u-e-n-t-i-a-l/#sequential","text":"val SEQUENTIAL: WorkerId","title":"SEQUENTIAL"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/segment/","text":"tempest / app.cash.tempest / WorkerId / segment segment \u00b6 val segment: Int A segment to be scanned by a particular worker. Each worker should use a different value for Segment. Segments are zero-based, so the first number is always 0.","title":"segment - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/segment/#segment","text":"val segment: Int A segment to be scanned by a particular worker. Each worker should use a different value for Segment. Segments are zero-based, so the first number is always 0.","title":"segment"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/total-segments/","text":"tempest / app.cash.tempest / WorkerId / totalSegments totalSegments \u00b6 val totalSegments: Int The total number of segments for the parallel scan. This value must be the same as the number of workers that your application will use.","title":"totalSegments - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-worker-id/total-segments/#totalsegments","text":"val totalSegments: Int The total number of segments for the parallel scan. This value must be the same as the number of workers that your application will use.","title":"totalSegments"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/","text":"tempest / app.cash.tempest / WritingPager WritingPager \u00b6 class WritingPager<T> A control flow abstraction for paging transactional writes. Types \u00b6 Name Summary Handler interface Handler<T> Constructors \u00b6 Name Summary <init> WritingPager(db: LogicalDb , maxTransactionItems: Int , handler: WritingPager.Handler < T >, updates: List < T >) A control flow abstraction for paging transactional writes. Properties \u00b6 Name Summary remainingUpdates val remainingUpdates: List < T > A snapshot of the elements yet to be updated. updatedCount var updatedCount: Int The number of updates successfully applied. Functions \u00b6 Name Summary execute fun execute(): Unit","title":"WritingPager - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/#writingpager","text":"class WritingPager<T> A control flow abstraction for paging transactional writes.","title":"WritingPager"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/#types","text":"Name Summary Handler interface Handler<T>","title":"Types"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/#constructors","text":"Name Summary <init> WritingPager(db: LogicalDb , maxTransactionItems: Int , handler: WritingPager.Handler < T >, updates: List < T >) A control flow abstraction for paging transactional writes.","title":"Constructors"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/#properties","text":"Name Summary remainingUpdates val remainingUpdates: List < T > A snapshot of the elements yet to be updated. updatedCount var updatedCount: Int The number of updates successfully applied.","title":"Properties"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/#functions","text":"Name Summary execute fun execute(): Unit","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-init-/","text":"tempest / app.cash.tempest / WritingPager / <init> <init> \u00b6 WritingPager(db: LogicalDb , maxTransactionItems: Int , handler: WritingPager.Handler < T >, updates: List < T >) A control flow abstraction for paging transactional writes.","title":" init "},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-init-/#init","text":"WritingPager(db: LogicalDb , maxTransactionItems: Int , handler: WritingPager.Handler < T >, updates: List < T >) A control flow abstraction for paging transactional writes.","title":"&lt;init&gt;"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/execute/","text":"tempest / app.cash.tempest / WritingPager / execute execute \u00b6 fun execute(): Unit","title":"execute - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/execute/#execute","text":"fun execute(): Unit","title":"execute"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/remaining-updates/","text":"tempest / app.cash.tempest / WritingPager / remainingUpdates remainingUpdates \u00b6 val remainingUpdates: List < T > A snapshot of the elements yet to be updated.","title":"remainingUpdates - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/remaining-updates/#remainingupdates","text":"val remainingUpdates: List < T > A snapshot of the elements yet to be updated.","title":"remainingUpdates"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/updated-count/","text":"tempest / app.cash.tempest / WritingPager / updatedCount updatedCount \u00b6 var updatedCount: Int The number of updates successfully applied.","title":"updatedCount - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/updated-count/#updatedcount","text":"var updatedCount: Int The number of updates successfully applied.","title":"updatedCount"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/","text":"tempest / app.cash.tempest / WritingPager / Handler Handler \u00b6 interface Handler<T> Functions \u00b6 Name Summary beforePage abstract fun beforePage(remainingUpdates: List < T >, maxTransactionItems: Int ): Int Invoked before each page with the full set of updates yet be processed. eachPage abstract fun eachPage(proceed: () -> Unit ): Unit Intercept each page\u2019s processing. Use this to decorate processing with metrics or retries. finishPage abstract fun finishPage(builder: TransactionWriteSet.Builder ): Unit Invoked after a page of items has been computed. item abstract fun item(builder: TransactionWriteSet.Builder , item: T ): Unit Invoked to update each item.","title":"Handler - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/#handler","text":"interface Handler<T>","title":"Handler"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/#functions","text":"Name Summary beforePage abstract fun beforePage(remainingUpdates: List < T >, maxTransactionItems: Int ): Int Invoked before each page with the full set of updates yet be processed. eachPage abstract fun eachPage(proceed: () -> Unit ): Unit Intercept each page\u2019s processing. Use this to decorate processing with metrics or retries. finishPage abstract fun finishPage(builder: TransactionWriteSet.Builder ): Unit Invoked after a page of items has been computed. item abstract fun item(builder: TransactionWriteSet.Builder , item: T ): Unit Invoked to update each item.","title":"Functions"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/before-page/","text":"tempest / app.cash.tempest / WritingPager / Handler / beforePage beforePage \u00b6 abstract fun beforePage(remainingUpdates: List < T >, maxTransactionItems: Int ): Int Invoked before each page with the full set of updates yet be processed. Parameters \u00b6 remainingUpdates - all remaining updates. This may be more than a single page of entities. Return the number of updates that fits in the current page.","title":"beforePage - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/before-page/#beforepage","text":"abstract fun beforePage(remainingUpdates: List < T >, maxTransactionItems: Int ): Int Invoked before each page with the full set of updates yet be processed.","title":"beforePage"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/before-page/#parameters","text":"remainingUpdates - all remaining updates. This may be more than a single page of entities. Return the number of updates that fits in the current page.","title":"Parameters"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/each-page/","text":"tempest / app.cash.tempest / WritingPager / Handler / eachPage eachPage \u00b6 abstract fun eachPage(proceed: () -> Unit ): Unit Intercept each page\u2019s processing. Use this to decorate processing with metrics or retries.","title":"eachPage - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/each-page/#eachpage","text":"abstract fun eachPage(proceed: () -> Unit ): Unit Intercept each page\u2019s processing. Use this to decorate processing with metrics or retries.","title":"eachPage"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/finish-page/","text":"tempest / app.cash.tempest / WritingPager / Handler / finishPage finishPage \u00b6 abstract fun finishPage(builder: TransactionWriteSet.Builder ): Unit Invoked after a page of items has been computed.","title":"finishPage - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/finish-page/#finishpage","text":"abstract fun finishPage(builder: TransactionWriteSet.Builder ): Unit Invoked after a page of items has been computed.","title":"finishPage"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/item/","text":"tempest / app.cash.tempest / WritingPager / Handler / item item \u00b6 abstract fun item(builder: TransactionWriteSet.Builder , item: T ): Unit Invoked to update each item.","title":"item - Tempest"},{"location":"0.x/tempest/app.cash.tempest/-writing-pager/-handler/item/#item","text":"abstract fun item(builder: TransactionWriteSet.Builder , item: T ): Unit Invoked to update each item.","title":"item"},{"location":"guide/batch_read_write/","text":"Batch Load \u00b6 LogicalDb lets you batchLoad multiple items from one or more tables using their primary keys. val musicDb : MusicDb fun loadPlaylistTracks ( playlist : PlaylistInfo ): Set < AlbumTrack > { val results = musicDb . batchLoad ( playlist . track_tokens , // [ AlbumTrack.Key(\"ALBUM_1\", track_number = 1), AlbumTrack.Key(\"ALBUM_354\", 12), AlbumTrack.Key(\"ALBUM_23\", 9) ] consistentReads = ConsistentReads . EVENTUAL , retryStrategy = DefaultBatchLoadRetryStrategy () ) return results . getItems < AlbumTrack >() } Batch load does not return items in any particular order In order to minimize response latency, BatchGetItem retrieves items in parallel. When designing your application, keep in mind that DynamoDB does not return items in any particular order. To help parse the response by item, include the primary key values for the items in your request in the ProjectionExpression parameter. If a requested item does not exist, it is not returned in the result. Requests for nonexistent items consume the minimum read capacity units according to the type of read. Batch Write \u00b6 LogicalDb lets you batch write and delete multiple items in multiple tables. Batch writes do not take condition expression With BatchWriteItem, you can efficiently write or delete large amounts of data, such as from Amazon EMR, or copy data from another database into DynamoDB. In order to improve performance with these large-scale operations, BatchWriteItem does not behave in the same way as individual PutItem and DeleteItem calls would. For example, you cannot specify conditions on individual put and delete requests, and BatchWriteItem does not return deleted items in the response. batchWrite does not provide transaction guarantees. Callers should always check the returned BatchWriteResult because this method returns normally even if some writes were not performed. Batch writes could be partially successful The individual PutItem and DeleteItem operations specified in BatchWriteItem are atomic; however BatchWriteItem as a whole is not. If any requested operations fail because the table\u2019s provisioned throughput is exceeded or an internal processing failure occurs, the failed operations are returned in the UnprocessedItems response parameter. val musicDb : MusicDb fun backfill ( albumTracksToSave : List < AlbumTrack >, albumTracksToDelete : List < AlbumTrack . Key > ): Boolean { val writeSet = BatchWriteSet . Builder () . clobber ( albumTracksToSave ) . delete ( albumTracksToDelete ) . build () val result = musicDb . batchWrite ( writeSet , retryStrategy = DefaultBatchWriteRetryStrategy () ) return result . isSuccessful }","title":"Batch Read & Write"},{"location":"guide/batch_read_write/#batch-load","text":"LogicalDb lets you batchLoad multiple items from one or more tables using their primary keys. val musicDb : MusicDb fun loadPlaylistTracks ( playlist : PlaylistInfo ): Set < AlbumTrack > { val results = musicDb . batchLoad ( playlist . track_tokens , // [ AlbumTrack.Key(\"ALBUM_1\", track_number = 1), AlbumTrack.Key(\"ALBUM_354\", 12), AlbumTrack.Key(\"ALBUM_23\", 9) ] consistentReads = ConsistentReads . EVENTUAL , retryStrategy = DefaultBatchLoadRetryStrategy () ) return results . getItems < AlbumTrack >() } Batch load does not return items in any particular order In order to minimize response latency, BatchGetItem retrieves items in parallel. When designing your application, keep in mind that DynamoDB does not return items in any particular order. To help parse the response by item, include the primary key values for the items in your request in the ProjectionExpression parameter. If a requested item does not exist, it is not returned in the result. Requests for nonexistent items consume the minimum read capacity units according to the type of read.","title":"Batch Load"},{"location":"guide/batch_read_write/#batch-write","text":"LogicalDb lets you batch write and delete multiple items in multiple tables. Batch writes do not take condition expression With BatchWriteItem, you can efficiently write or delete large amounts of data, such as from Amazon EMR, or copy data from another database into DynamoDB. In order to improve performance with these large-scale operations, BatchWriteItem does not behave in the same way as individual PutItem and DeleteItem calls would. For example, you cannot specify conditions on individual put and delete requests, and BatchWriteItem does not return deleted items in the response. batchWrite does not provide transaction guarantees. Callers should always check the returned BatchWriteResult because this method returns normally even if some writes were not performed. Batch writes could be partially successful The individual PutItem and DeleteItem operations specified in BatchWriteItem are atomic; however BatchWriteItem as a whole is not. If any requested operations fail because the table\u2019s provisioned throughput is exceeded or an internal processing failure occurs, the failed operations are returned in the UnprocessedItems response parameter. val musicDb : MusicDb fun backfill ( albumTracksToSave : List < AlbumTrack >, albumTracksToDelete : List < AlbumTrack . Key > ): Boolean { val writeSet = BatchWriteSet . Builder () . clobber ( albumTracksToSave ) . delete ( albumTracksToDelete ) . build () val result = musicDb . batchWrite ( writeSet , retryStrategy = DefaultBatchWriteRetryStrategy () ) return result . isSuccessful }","title":"Batch Write"},{"location":"guide/crud/","text":"We\u2019ve written some examples that demonstrate how to solve common problems with Tempest. Read through them to learn about how everything works together. interface MusicTable : LogicalTable < MusicItem > { val albumInfo : InlineView < AlbumInfo . Key , AlbumInfo > val albumTracks : InlineView < AlbumTrack . Key , AlbumTrack > } data class AlbumInfo ( @Attribute ( name = \"partition_key\" ) val album_token : String , val album_name : String , val release_date : LocalDate , val genre_name : String ) { @Attribute ( prefix = \"INFO_\" ) val sort_key : String = \"\" data class Key ( val album_token : String ) { val sort_key : String = \"\" } } data class AlbumTrack ( @Attribute ( name = \"partition_key\" ) val album_token : String , @Attribute ( name = \"sort_key\" , prefix = \"TRACK_\" ) val track_token : String , val track_name : String , val track_run_length : Duration ) { data class Key ( val album_token : String , val track_token : String ) } Read \u00b6 Use load() to read a value. private val table : MusicTable fun getAlbumTitle ( albumToken : String ): String ? { val albumInfo = table . albumInfo . load ( AlbumInfo . Key ( albumToken )) ?: return null return albumInfo . album_title } DynamoDB is eventually consistent by default For actions that only read data, this is usually fine! Once the read completes it could be updated anyway, so whether the read reflects very recent writes is typically insignificant. If your read immediately follows a write of the same item, you should use a strongly consistent read to ensure your read reflects the write. // Write an item. val item : AlbumInfo musicTable . albumInfo . save ( item ) // Read that item. val itemRead = musicTable . albumInfo . load ( item . key ) // Note that the value we just read might be older than the value we wrote. If you need to read your writes, you may perform a strongly consistent read at a higher latency. private val table : MusicTable fun getAlbumTitle ( albumToken : String ): String ? { val albumInfo = table . albumInfo . load ( AlbumInfo . Key ( albumToken ), consistentReads = ConsistentReads . CONSISTENT ) ?: return null return albumInfo . album_title } Update \u00b6 By default, writes are unconditional. When there is a conflict, the last writer wins. private val table : MusicTable fun addAlbum ( albumInfo : AlbumInfo ) { musicTable . albumInfo . save ( albumInfo ) } To prevent lost updates across concurrent writes, you may specify a condition expression. If the condition expression evaluates to true, the operation is applied; otherwise, the operation is rolled back. private val table : MusicTable fun addAlbum ( albumInfo : AlbumInfo ) { musicTable . albumInfo . save ( albumInfo , ifNotExist ()) } fun ifNotExist (): DynamoDBSaveExpression { return DynamoDBSaveExpression () . withExpectedEntry ( \"partition_key\" , ExpectedAttributeValue (). withExists ( false )) } Delete \u00b6 Use delete() to delete a value by key. private val table : MusicTable fun deleteAlbum ( albumToken : String ) { musicTable . albumInfo . delete ( AlbumInfo . Key ( albumToken )) } Similarly, you can add a condition expression to the delete operation. private val table : MusicTable fun deleteAlbum ( albumToken : String ) { musicTable . albumInfo . delete ( AlbumInfo . Key ( albumToken ), ifExist ()) } fun ifExist (): DynamoDBSaveExpression { return DynamoDBSaveExpression () . withExpectedEntry ( \"partition_key\" , ExpectedAttributeValue (). withExists ( true )) }","title":"CRUD"},{"location":"guide/crud/#read","text":"Use load() to read a value. private val table : MusicTable fun getAlbumTitle ( albumToken : String ): String ? { val albumInfo = table . albumInfo . load ( AlbumInfo . Key ( albumToken )) ?: return null return albumInfo . album_title } DynamoDB is eventually consistent by default For actions that only read data, this is usually fine! Once the read completes it could be updated anyway, so whether the read reflects very recent writes is typically insignificant. If your read immediately follows a write of the same item, you should use a strongly consistent read to ensure your read reflects the write. // Write an item. val item : AlbumInfo musicTable . albumInfo . save ( item ) // Read that item. val itemRead = musicTable . albumInfo . load ( item . key ) // Note that the value we just read might be older than the value we wrote. If you need to read your writes, you may perform a strongly consistent read at a higher latency. private val table : MusicTable fun getAlbumTitle ( albumToken : String ): String ? { val albumInfo = table . albumInfo . load ( AlbumInfo . Key ( albumToken ), consistentReads = ConsistentReads . CONSISTENT ) ?: return null return albumInfo . album_title }","title":"Read"},{"location":"guide/crud/#update","text":"By default, writes are unconditional. When there is a conflict, the last writer wins. private val table : MusicTable fun addAlbum ( albumInfo : AlbumInfo ) { musicTable . albumInfo . save ( albumInfo ) } To prevent lost updates across concurrent writes, you may specify a condition expression. If the condition expression evaluates to true, the operation is applied; otherwise, the operation is rolled back. private val table : MusicTable fun addAlbum ( albumInfo : AlbumInfo ) { musicTable . albumInfo . save ( albumInfo , ifNotExist ()) } fun ifNotExist (): DynamoDBSaveExpression { return DynamoDBSaveExpression () . withExpectedEntry ( \"partition_key\" , ExpectedAttributeValue (). withExists ( false )) }","title":"Update"},{"location":"guide/crud/#delete","text":"Use delete() to delete a value by key. private val table : MusicTable fun deleteAlbum ( albumToken : String ) { musicTable . albumInfo . delete ( AlbumInfo . Key ( albumToken )) } Similarly, you can add a condition expression to the delete operation. private val table : MusicTable fun deleteAlbum ( albumToken : String ) { musicTable . albumInfo . delete ( AlbumInfo . Key ( albumToken ), ifExist ()) } fun ifExist (): DynamoDBSaveExpression { return DynamoDBSaveExpression () . withExpectedEntry ( \"partition_key\" , ExpectedAttributeValue (). withExists ( true )) }","title":"Delete"},{"location":"guide/data_modeling/","text":"Warning For an RDBMS, you can create a normalized data model without thinking about access patterns. You can then extend it later when new questions and query requirements arise. By contrast, in Amazon DynamoDB, you shouldn\u2019t start designing your schema until you know the questions that it needs to answer. Understanding the business problems and the application use cases up front is absolutely essential. DynamoDB Approach \u00b6 As explained in the project overview , DynamoDB approaches data modeling differently from relational databases. Access Patterns \u00b6 In each DynamoDB table, data is organized for locality of expected access patterns. This gives you excellent performance if the structure of the data and access patterns agree and terrible performance otherwise. To design a DynamoDB table that scales efficiently, you must first identify the access patterns required by the business logic. Suppose we are building a music library with this entity model. erDiagram ALBUM ||--|{ TRACK : contains ALBUM }|..|| ARTIST : contains PLAYLIST ||--|{ TRACK : contains This music library needs to support the following access patterns. The Most Common Access Patterns 1 Load album and its tracks by album token 2 Load track by track token 3 List tracks in a playlist 4 List albums by artist name 5 Find album by album title 6 Find track by track title ... Efficient Schema \u00b6 An efficient schema that keeps related data in close proximity has a major impact on cost and performance. Instead of distributing related data items across multiple tables, you should keep related items in your table as close together as possible. Typically, this means storing related rows in the same table, and with the same partition key. Within a partition key, related rows should share a prefix of the sort key. We can design an efficient schema using these patterns: Denormalization : Schema flexibility lets DynamoDB store structured data, such as lists, sets, and nested objects, in a single item. Composite key aggregation : Deliberate key design puts related entities close together. Sort order : Related items can be grouped together and queried efficiently if their key design causes them to sort together. Global secondary indexes : By creating specific global secondary indexes, you can enable different queries than your main table can support, and that are still fast and inexpensive. Adjacency list : Adjacency lists are a design pattern that is useful for modeling many-to-many relationships. More generally, they provide a way to represent graph data (nodes and edges). Primary Key Attributes partition_key sort_key ALBUM_1 INFO album_title album_artiest release_date genre The Dark Side of the Moon Pink Floyd 1973-03-01 Progressive rock ALBUM_1 TRACK_1 track_title run_length Speak to Me PT1M13S ALBUM_1 TRACK_2 track_title run_length Breathe PT2M43S ALBUM_1 TRACK_3 track_title run_length On the Run PT3M36S ... ALBUM_2 INFO album_title album_artiest release_date genre The Wall Pink Floyd 1979-11-30 Progressive rock ALBUM_2 TRACK_1 track_title run_length In the Flesh? PT3M20S ... PLAYLIST_1 INFO playlist_name playlist_size playlist_tracks playlist_version Psychedelic Rock Essentials 100 ALBUM_1/TRACK_1, ALBUM_1322/TRACK_9, ALBUM_3423/TRACK_3, ALBUM_84/TRACK_10, ALBUM_2/TRACK_5, ... 12 ... This table uses a composite primary key , (parition_key, sort_key) , to identify each item. The key (\"ALBUM_1\", \"INFO\") identifies ALBUM_1 \u2018s metadata. The key (\"ALBUM_1\", \"TRACK_1\") identifies ALBUM_1 \u2018s first track. The key (\"PLAYLIST_1\", \"INFO\") identifies PLAYLIST_1 \u2018s content. It uses secondary indexes to answer additional queries. GSI Primary Key Projected Attributes artist_name partition_key Pink Floyd ALBUM_1 album_title sort_key release_date genre The Dark Side of the Moon INFO 1973-03-01 Progressive rock Pink Floyd ALBUM_2 album_title sort_key release_date genre The Wall INFO 1979-11-30 Progressive rock ... The Beatles ALBUM_232 album_title sort_key release_date genre Revolver INFO 1966-06-21 Rock ... This global secondary index groups AlbumInfo by artist_name and sorts them by the primary index parition_key . Tempest \u00b6 Tempest lets you define strongly typed data models on top of your DynamoDBMapper classes. interface MusicDb : LogicalDb { val music : MusicTable } interface MusicTable : LogicalTable < MusicItem > { val albumInfo : InlineView < AlbumInfo . Key , AlbumInfo > val albumTracks : InlineView < AlbumTrack . Key , AlbumTrack > val playlistInfo : InlineView < PlaylistInfo . Key , PlaylistInfo > // Global Secondary Indexes. val albumInfoByGenre : SecondaryIndex < AlbumInfo . GenreIndexOffset , AlbumInfo > val albumInfoByArtist : SecondaryIndex < AlbumInfo . ArtistIndexOffset , AlbumInfo > // Local Secondary Indexes. val albumTracksByTitle : SecondaryIndex < AlbumTrack . TitleIndexOffset , AlbumTrack > } @DynamoDBTable ( tableName = \"music_items\" ) class MusicItem { // Primary key. @DynamoDBHashKey @DynamoDBIndexRangeKey ( globalSecondaryIndexNames = [ \"genre_album_index\" , \"artist_album_index\" ]) var partition_key : String ? = null @DynamoDBRangeKey var sort_key : String ? = null // Attributes... } Tempest has these components: Logical DB Logical tables (1 to 1 with your DynamoDBMapper classes) Inline views Key type Item type Secondary indexes Offset type Item type Logical DB \u00b6 A LogicalDb is a collection of tables that implement the DynamoDB best practice of putting multiple item types into the same storage table. This makes it possible to perform aggregate operations and transactions on those item types. For example, you can batch load up to 100 items in a single request. val items = musicDb . batchLoad ( AlbumTrack . Key ( \"ALBUM_1\" , \"TRACK_5\" ), AlbumTrack . Key ( \"ALBUM_2\" , \"TRACK_3\" ), PlaylistInfo . Key ( \"PLAYLIST_1\" )) To create a LogicalDb , you need to pass in an instance of DynamoDBMapper . val client : AmazonDynamoDB = AmazonDynamoDBClientBuilder . standard (). build () val mapper : DynamoDBMapper = DynamoDBMapper ( client ) val db : MusicDb = LogicalDb ( mapper ) Optional Configuration \u00b6 When you create an instance of DynamoDBMapper, it has certain default behaviors; you can override these defaults by using the DynamoDBMapperConfig class. The following code snippet creates a DynamoDBMapper with custom settings: val client = AmazonDynamoDBClientBuilder . standard (). build () val mapperConfig = DynamoDBMapperConfig . builder () . withSaveBehavior ( SaveBehavior . CLOBBER ) . withConsistentReads ( ConsistentReads . CONSISTENT ) . withTableNameOverride ( null ) . withPaginationLoadingStrategy ( PaginationLoadingStrategy . EAGER_LOADING ) . build () val mapper = DynamoDBMapper ( client , mapperConfig ) val musicDb : MusicDb = LogicalDB ( mapper ) For more information, see the DynamoDBMapper documentation Logical Table \u00b6 A LogicalTable is a collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes. Inline View \u00b6 An InlineView lets you perform CRUD operations, queries, and scans on an entity type. interface MusicTable : LogicalTable < MusicItem > { val albumInfo : InlineView < AlbumInfo . Key , AlbumInfo > } data class AlbumInfo . Key ( val album_token : String ) { val sort_key : String = \"\" } data class AlbumInfo ( @Attribute ( name = \"partition_key\" ) val album_token : String , val artist_name : String , val release_date : LocalDate , val genre_name : String ) { @Attribute ( prefix = \"INFO_\" ) val sort_key : String = \"\" } The albumInfo view is a type-safe way to access AlbumInfo entities: Primary Key Attributes partition_key sort_key ALBUM_1 INFO album_title album_artiest release_date genre The Dark Side of the Moon Pink Floyd 1973-03-01 Progressive rock ... ALBUM_2 INFO album_title album_artiest release_date genre The Wall Pink Floyd 1979-11-30 Progressive rock ... Prefixes are 1:1 with Types A LogicalTable can have multiple InlineView s. Tempest requires you to declare a prefix on the sort key in each entity type. It uses the prefix to determine the entity type. Prefix Type INFO_ AlbumInfo TRACK_ AlbumTrack interface MusicTable : LogicalTable < MusicItem > { val albumInfo : InlineView < AlbumInfo . Key , AlbumInfo > val albumTracks : InlineView < AlbumTrack . Key , AlbumTrack > } data class AlbumInfo ( @Attribute ( name = \"partition_key\" ) val album_token : String , // ... ) { @Attribute ( prefix = \"INFO_\" ) val sort_key : String = \"\" } data class AlbumTrack ( @Attribute ( name = \"partition_key\" ) val album_token : String , @Attribute ( name = \"sort_key\" , prefix = \"TRACK_\" ) val track_token : String , // ... ) Secondary Index \u00b6 An SecondaryIndex lets you perform queries, and scans on an entity type. interface MusicTable : LogicalTable < MusicItem > { val albumInfoByArtist : SecondaryIndex < AlbumInfo . ArtistIndexOffset , AlbumInfo > } data class AlbumInfo ( @Attribute ( name = \"partition_key\" ) val album_token : String , val artist_name : String , val release_date : LocalDate , val genre_name : String ) { @Attribute ( prefix = \"INFO_\" ) val sort_key : String = \"\" @ForIndex ( \"artist_album_index\" ) data class ArtistIndexOffset ( val artist_name : String , val album_token : String ? = null , // To uniquely identify an item in pagination. val sort_key : String ? = null ) } DynamoDB secondary indexes allows duplicate values. In order to uniquely identify an item in pagination, a secondary index offset type needs to include the primary index partition key and sort key in addition to the secondary index partition key and sort key. Secondary index offset types are also required to have a @ForIndex annotation that tells Tempest the index name. Properties are always mapped by name Our secondary index data class properties have the exact same name as the properties in our DynamoDB mapper class. Tempest uses name equality to bind indexes, keys, and items to the DynamoDB mapper class. Each Tempest type represents a different logical subset of the available attributes. The mapper class is just the union of the fields in each item, key, and secondary index. MusicItem AlbumInfo.Key AlbumInfo.ArtistIndexOffset AlbumInfo AlbumTrack.Key AlbumTrack partition_key partition_key partition_key partition_key partition_key partition_key sort_key sort_key sort_key sort_key sort_key sort_key album_title album_title artist_name artist_name artist_name release_date release_date genre genre track_title track_title run_length run_length Custom Attribute Types \u00b6 Tempest uses DynamoDBMapper to encode and decode entities. DynamoDBMapper supports these primitive Java types . You may use DynamoDBTypeConverter to support custom attribute types . @DynamoDBTable ( tableName = \"music_items\" ) class MusicItem { // ... @DynamoDBAttribute @DynamoDBTypeConverted ( converter = LocalDateTypeConverter :: class ) var release_date : LocalDate ? = null // ... } class LocalDateTypeConverter : DynamoDBTypeConverter < String , LocalDate > { override fun unconvert ( string : String ): LocalDate { return LocalDate . parse ( string ) } override fun convert ( localDate : LocalDate ): String { return localDate . toString () } }","title":"Data Modeling"},{"location":"guide/data_modeling/#dynamodb-approach","text":"As explained in the project overview , DynamoDB approaches data modeling differently from relational databases.","title":"DynamoDB Approach"},{"location":"guide/data_modeling/#access-patterns","text":"In each DynamoDB table, data is organized for locality of expected access patterns. This gives you excellent performance if the structure of the data and access patterns agree and terrible performance otherwise. To design a DynamoDB table that scales efficiently, you must first identify the access patterns required by the business logic. Suppose we are building a music library with this entity model. erDiagram ALBUM ||--|{ TRACK : contains ALBUM }|..|| ARTIST : contains PLAYLIST ||--|{ TRACK : contains This music library needs to support the following access patterns. The Most Common Access Patterns 1 Load album and its tracks by album token 2 Load track by track token 3 List tracks in a playlist 4 List albums by artist name 5 Find album by album title 6 Find track by track title ...","title":"Access Patterns"},{"location":"guide/data_modeling/#efficient-schema","text":"An efficient schema that keeps related data in close proximity has a major impact on cost and performance. Instead of distributing related data items across multiple tables, you should keep related items in your table as close together as possible. Typically, this means storing related rows in the same table, and with the same partition key. Within a partition key, related rows should share a prefix of the sort key. We can design an efficient schema using these patterns: Denormalization : Schema flexibility lets DynamoDB store structured data, such as lists, sets, and nested objects, in a single item. Composite key aggregation : Deliberate key design puts related entities close together. Sort order : Related items can be grouped together and queried efficiently if their key design causes them to sort together. Global secondary indexes : By creating specific global secondary indexes, you can enable different queries than your main table can support, and that are still fast and inexpensive. Adjacency list : Adjacency lists are a design pattern that is useful for modeling many-to-many relationships. More generally, they provide a way to represent graph data (nodes and edges). Primary Key Attributes partition_key sort_key ALBUM_1 INFO album_title album_artiest release_date genre The Dark Side of the Moon Pink Floyd 1973-03-01 Progressive rock ALBUM_1 TRACK_1 track_title run_length Speak to Me PT1M13S ALBUM_1 TRACK_2 track_title run_length Breathe PT2M43S ALBUM_1 TRACK_3 track_title run_length On the Run PT3M36S ... ALBUM_2 INFO album_title album_artiest release_date genre The Wall Pink Floyd 1979-11-30 Progressive rock ALBUM_2 TRACK_1 track_title run_length In the Flesh? PT3M20S ... PLAYLIST_1 INFO playlist_name playlist_size playlist_tracks playlist_version Psychedelic Rock Essentials 100 ALBUM_1/TRACK_1, ALBUM_1322/TRACK_9, ALBUM_3423/TRACK_3, ALBUM_84/TRACK_10, ALBUM_2/TRACK_5, ... 12 ... This table uses a composite primary key , (parition_key, sort_key) , to identify each item. The key (\"ALBUM_1\", \"INFO\") identifies ALBUM_1 \u2018s metadata. The key (\"ALBUM_1\", \"TRACK_1\") identifies ALBUM_1 \u2018s first track. The key (\"PLAYLIST_1\", \"INFO\") identifies PLAYLIST_1 \u2018s content. It uses secondary indexes to answer additional queries. GSI Primary Key Projected Attributes artist_name partition_key Pink Floyd ALBUM_1 album_title sort_key release_date genre The Dark Side of the Moon INFO 1973-03-01 Progressive rock Pink Floyd ALBUM_2 album_title sort_key release_date genre The Wall INFO 1979-11-30 Progressive rock ... The Beatles ALBUM_232 album_title sort_key release_date genre Revolver INFO 1966-06-21 Rock ... This global secondary index groups AlbumInfo by artist_name and sorts them by the primary index parition_key .","title":"Efficient Schema"},{"location":"guide/data_modeling/#tempest","text":"Tempest lets you define strongly typed data models on top of your DynamoDBMapper classes. interface MusicDb : LogicalDb { val music : MusicTable } interface MusicTable : LogicalTable < MusicItem > { val albumInfo : InlineView < AlbumInfo . Key , AlbumInfo > val albumTracks : InlineView < AlbumTrack . Key , AlbumTrack > val playlistInfo : InlineView < PlaylistInfo . Key , PlaylistInfo > // Global Secondary Indexes. val albumInfoByGenre : SecondaryIndex < AlbumInfo . GenreIndexOffset , AlbumInfo > val albumInfoByArtist : SecondaryIndex < AlbumInfo . ArtistIndexOffset , AlbumInfo > // Local Secondary Indexes. val albumTracksByTitle : SecondaryIndex < AlbumTrack . TitleIndexOffset , AlbumTrack > } @DynamoDBTable ( tableName = \"music_items\" ) class MusicItem { // Primary key. @DynamoDBHashKey @DynamoDBIndexRangeKey ( globalSecondaryIndexNames = [ \"genre_album_index\" , \"artist_album_index\" ]) var partition_key : String ? = null @DynamoDBRangeKey var sort_key : String ? = null // Attributes... } Tempest has these components: Logical DB Logical tables (1 to 1 with your DynamoDBMapper classes) Inline views Key type Item type Secondary indexes Offset type Item type","title":"Tempest"},{"location":"guide/data_modeling/#logical-db","text":"A LogicalDb is a collection of tables that implement the DynamoDB best practice of putting multiple item types into the same storage table. This makes it possible to perform aggregate operations and transactions on those item types. For example, you can batch load up to 100 items in a single request. val items = musicDb . batchLoad ( AlbumTrack . Key ( \"ALBUM_1\" , \"TRACK_5\" ), AlbumTrack . Key ( \"ALBUM_2\" , \"TRACK_3\" ), PlaylistInfo . Key ( \"PLAYLIST_1\" )) To create a LogicalDb , you need to pass in an instance of DynamoDBMapper . val client : AmazonDynamoDB = AmazonDynamoDBClientBuilder . standard (). build () val mapper : DynamoDBMapper = DynamoDBMapper ( client ) val db : MusicDb = LogicalDb ( mapper )","title":"Logical DB"},{"location":"guide/data_modeling/#optional-configuration","text":"When you create an instance of DynamoDBMapper, it has certain default behaviors; you can override these defaults by using the DynamoDBMapperConfig class. The following code snippet creates a DynamoDBMapper with custom settings: val client = AmazonDynamoDBClientBuilder . standard (). build () val mapperConfig = DynamoDBMapperConfig . builder () . withSaveBehavior ( SaveBehavior . CLOBBER ) . withConsistentReads ( ConsistentReads . CONSISTENT ) . withTableNameOverride ( null ) . withPaginationLoadingStrategy ( PaginationLoadingStrategy . EAGER_LOADING ) . build () val mapper = DynamoDBMapper ( client , mapperConfig ) val musicDb : MusicDb = LogicalDB ( mapper ) For more information, see the DynamoDBMapper documentation","title":"Optional Configuration"},{"location":"guide/data_modeling/#logical-table","text":"A LogicalTable is a collection of views on a DynamoDB table that makes it easy to model heterogeneous items using strongly typed data classes.","title":"Logical Table"},{"location":"guide/data_modeling/#inline-view","text":"An InlineView lets you perform CRUD operations, queries, and scans on an entity type. interface MusicTable : LogicalTable < MusicItem > { val albumInfo : InlineView < AlbumInfo . Key , AlbumInfo > } data class AlbumInfo . Key ( val album_token : String ) { val sort_key : String = \"\" } data class AlbumInfo ( @Attribute ( name = \"partition_key\" ) val album_token : String , val artist_name : String , val release_date : LocalDate , val genre_name : String ) { @Attribute ( prefix = \"INFO_\" ) val sort_key : String = \"\" } The albumInfo view is a type-safe way to access AlbumInfo entities: Primary Key Attributes partition_key sort_key ALBUM_1 INFO album_title album_artiest release_date genre The Dark Side of the Moon Pink Floyd 1973-03-01 Progressive rock ... ALBUM_2 INFO album_title album_artiest release_date genre The Wall Pink Floyd 1979-11-30 Progressive rock ... Prefixes are 1:1 with Types A LogicalTable can have multiple InlineView s. Tempest requires you to declare a prefix on the sort key in each entity type. It uses the prefix to determine the entity type. Prefix Type INFO_ AlbumInfo TRACK_ AlbumTrack interface MusicTable : LogicalTable < MusicItem > { val albumInfo : InlineView < AlbumInfo . Key , AlbumInfo > val albumTracks : InlineView < AlbumTrack . Key , AlbumTrack > } data class AlbumInfo ( @Attribute ( name = \"partition_key\" ) val album_token : String , // ... ) { @Attribute ( prefix = \"INFO_\" ) val sort_key : String = \"\" } data class AlbumTrack ( @Attribute ( name = \"partition_key\" ) val album_token : String , @Attribute ( name = \"sort_key\" , prefix = \"TRACK_\" ) val track_token : String , // ... )","title":"Inline View"},{"location":"guide/data_modeling/#secondary-index","text":"An SecondaryIndex lets you perform queries, and scans on an entity type. interface MusicTable : LogicalTable < MusicItem > { val albumInfoByArtist : SecondaryIndex < AlbumInfo . ArtistIndexOffset , AlbumInfo > } data class AlbumInfo ( @Attribute ( name = \"partition_key\" ) val album_token : String , val artist_name : String , val release_date : LocalDate , val genre_name : String ) { @Attribute ( prefix = \"INFO_\" ) val sort_key : String = \"\" @ForIndex ( \"artist_album_index\" ) data class ArtistIndexOffset ( val artist_name : String , val album_token : String ? = null , // To uniquely identify an item in pagination. val sort_key : String ? = null ) } DynamoDB secondary indexes allows duplicate values. In order to uniquely identify an item in pagination, a secondary index offset type needs to include the primary index partition key and sort key in addition to the secondary index partition key and sort key. Secondary index offset types are also required to have a @ForIndex annotation that tells Tempest the index name. Properties are always mapped by name Our secondary index data class properties have the exact same name as the properties in our DynamoDB mapper class. Tempest uses name equality to bind indexes, keys, and items to the DynamoDB mapper class. Each Tempest type represents a different logical subset of the available attributes. The mapper class is just the union of the fields in each item, key, and secondary index. MusicItem AlbumInfo.Key AlbumInfo.ArtistIndexOffset AlbumInfo AlbumTrack.Key AlbumTrack partition_key partition_key partition_key partition_key partition_key partition_key sort_key sort_key sort_key sort_key sort_key sort_key album_title album_title artist_name artist_name artist_name release_date release_date genre genre track_title track_title run_length run_length","title":"Secondary Index"},{"location":"guide/data_modeling/#custom-attribute-types","text":"Tempest uses DynamoDBMapper to encode and decode entities. DynamoDBMapper supports these primitive Java types . You may use DynamoDBTypeConverter to support custom attribute types . @DynamoDBTable ( tableName = \"music_items\" ) class MusicItem { // ... @DynamoDBAttribute @DynamoDBTypeConverted ( converter = LocalDateTypeConverter :: class ) var release_date : LocalDate ? = null // ... } class LocalDateTypeConverter : DynamoDBTypeConverter < String , LocalDate > { override fun unconvert ( string : String ): LocalDate { return LocalDate . parse ( string ) } override fun convert ( localDate : LocalDate ): String { return localDate . toString () } }","title":"Custom Attribute Types"},{"location":"guide/dynamodb_resources/","text":"Official Developer Guide Advanced Design Patterns for DynamoDB \u2014 Slide Amazon DynamoDB Under the Hood \u2014 Slide The What, Why, and When of Single-Table Design with DynamoDB","title":"DynamoDB Resources"},{"location":"guide/getting_started/","text":"Tip In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility. To learn more about DynamoDB, check out the official developer guide . Get Tempest \u00b6 First, add Tempest to your project. With Gradle: dependencies { implementation \"app.cash.tempest:tempest:0.1.0\" } Start Coding \u00b6 Let\u2019s build a URL shortener with the following features: Creating custom aliases from a short URL to a destination URL. Redirecting existing short URLs to destination URLs. We express it like this in Kotlin. interface UrlShortener { /** * Creates a custom alias from [shortUrl] to [destinationUrl]. * @return false if [shortUrl] is taken. */ fun shorten ( shortUrl : String , destinationUrl : String ): Boolean /** * Redirects [shortUrl] to its destination. * @return null if not found. */ fun redirect ( shortUrl : String ): String ? } We will store URL aliases in the following table. Primary Key Attributes short_url SquareCLA destination_url https://docs.google.com/forms/d/e/1FAIpQLSeRVQ35-gq2vdSxD1kdh7CJwRdjmUA0EZ9gRXaWYoUeKPZEQQ/viewform?formkey=dDViT2xzUHAwRkI3X3k5Z0lQM091OGc6MQ&ndplr=1 KindleWireless destination_url http://www.amazon.com/Kindle-Wireless-Reading-Display-Globally/dp/B003FSUDM4/ref=amb_link_353259562_2?pf_rd_m=ATVPDKIKX0DER&pf_rd_s=center-10&pf_rd_r=11EYKTN682A79T370AM3&pf_rd_t=201&pf_rd_p=1270985982&pf_rd_i=B002Y27P3M BestUrlShortener destination_url https://www.google.com/search?q=best+url+shortener&oq=best+url+shortener&aqs=chrome..69i57j69i64l2.8705j0j1&sourceid=chrome&ie=UTF-8 ... To access this table in Kotlin, model it using DynamoDBMapper . // Note: this POJO is not type-safe because its attributes are nullable and mutable. @DynamoDBTable ( tableName = \"alias_items\" ) class AliasItem { @DynamoDBHashKey var short_url : String ? = null @DynamoDBAttribute var destination_url : String ? = null } Tempest lets you interact with AliasItem using strongly typed data classes. interface UrlShortenerDb : LogicalDb { val aliasTable : AliasTable } interface AliasTable : LogicalTable < AliasItem > { val aliases : InlineView < Alias . Key , Alias > } data class Alias ( val short_url : String , val destination_url : String ) { data class Key ( val short_url : String ) } Let\u2019s put everything together. class RealUrlShortener ( private val table : AliasTable ): UrlShortener { override fun shorten ( shortUrl : String , destinationUrl : String ): Boolean { val item = Alias ( shortUrl , destinationUrl ) val ifNotExist = DynamoDBSaveExpression () . withExpectedEntry ( \"short_url\" , ExpectedAttributeValue (). withExists ( false )) return try { table . aliases . save ( item , ifNotExist ) true } catch ( e : ConditionalCheckFailedException ) { println ( \"Failed to shorten $shortUrl because it already exists!\" ) false } } override fun redirect ( shortUrl : String ): String ? { val key = Alias . Key ( shortUrl ) return table . aliases . load ( key ) ?. destination_url } } fun main () { val client : AmazonDynamoDB = AmazonDynamoDBClientBuilder . standard (). build () val mapper : DynamoDBMapper = DynamoDBMapper ( client ) val db : UrlShortenerDb = LogicalDb ( mapper ) val urlShortener = RealUrlShortener ( db . aliasTable ) urlShortener . shorten ( \"tempest\" , \"https://cashapp.github.io/tempest\" ) }","title":"Getting Started"},{"location":"guide/getting_started/#get-tempest","text":"First, add Tempest to your project. With Gradle: dependencies { implementation \"app.cash.tempest:tempest:0.1.0\" }","title":"Get Tempest"},{"location":"guide/getting_started/#start-coding","text":"Let\u2019s build a URL shortener with the following features: Creating custom aliases from a short URL to a destination URL. Redirecting existing short URLs to destination URLs. We express it like this in Kotlin. interface UrlShortener { /** * Creates a custom alias from [shortUrl] to [destinationUrl]. * @return false if [shortUrl] is taken. */ fun shorten ( shortUrl : String , destinationUrl : String ): Boolean /** * Redirects [shortUrl] to its destination. * @return null if not found. */ fun redirect ( shortUrl : String ): String ? } We will store URL aliases in the following table. Primary Key Attributes short_url SquareCLA destination_url https://docs.google.com/forms/d/e/1FAIpQLSeRVQ35-gq2vdSxD1kdh7CJwRdjmUA0EZ9gRXaWYoUeKPZEQQ/viewform?formkey=dDViT2xzUHAwRkI3X3k5Z0lQM091OGc6MQ&ndplr=1 KindleWireless destination_url http://www.amazon.com/Kindle-Wireless-Reading-Display-Globally/dp/B003FSUDM4/ref=amb_link_353259562_2?pf_rd_m=ATVPDKIKX0DER&pf_rd_s=center-10&pf_rd_r=11EYKTN682A79T370AM3&pf_rd_t=201&pf_rd_p=1270985982&pf_rd_i=B002Y27P3M BestUrlShortener destination_url https://www.google.com/search?q=best+url+shortener&oq=best+url+shortener&aqs=chrome..69i57j69i64l2.8705j0j1&sourceid=chrome&ie=UTF-8 ... To access this table in Kotlin, model it using DynamoDBMapper . // Note: this POJO is not type-safe because its attributes are nullable and mutable. @DynamoDBTable ( tableName = \"alias_items\" ) class AliasItem { @DynamoDBHashKey var short_url : String ? = null @DynamoDBAttribute var destination_url : String ? = null } Tempest lets you interact with AliasItem using strongly typed data classes. interface UrlShortenerDb : LogicalDb { val aliasTable : AliasTable } interface AliasTable : LogicalTable < AliasItem > { val aliases : InlineView < Alias . Key , Alias > } data class Alias ( val short_url : String , val destination_url : String ) { data class Key ( val short_url : String ) } Let\u2019s put everything together. class RealUrlShortener ( private val table : AliasTable ): UrlShortener { override fun shorten ( shortUrl : String , destinationUrl : String ): Boolean { val item = Alias ( shortUrl , destinationUrl ) val ifNotExist = DynamoDBSaveExpression () . withExpectedEntry ( \"short_url\" , ExpectedAttributeValue (). withExists ( false )) return try { table . aliases . save ( item , ifNotExist ) true } catch ( e : ConditionalCheckFailedException ) { println ( \"Failed to shorten $shortUrl because it already exists!\" ) false } } override fun redirect ( shortUrl : String ): String ? { val key = Alias . Key ( shortUrl ) return table . aliases . load ( key ) ?. destination_url } } fun main () { val client : AmazonDynamoDB = AmazonDynamoDBClientBuilder . standard (). build () val mapper : DynamoDBMapper = DynamoDBMapper ( client ) val db : UrlShortenerDb = LogicalDb ( mapper ) val urlShortener = RealUrlShortener ( db . aliasTable ) urlShortener . shorten ( \"tempest\" , \"https://cashapp.github.io/tempest\" ) }","title":"Start Coding"},{"location":"guide/optimistic_locking/","text":"When two writers write to the same item at the same time, there is a conflict. By default, the last writer wins. To avoid conflicts in your application, check out these tools: Numeric attributes only : Atomic counters models numeric attributes that are incremented, unconditionally, without interfering with other write requests. Most use cases : Optimistic locking is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in Amazon DynamoDB. If you use this strategy, your database writes are protected from being overwritten by the writes of others, and vice versa. Global tables do not support optimistic locking DynamoDB global tables use a \u201clast writer wins\u201d reconciliation between concurrent updates. If you use global tables, last writer policy wins. So in this case, the locking strategy does not work as expected. Let\u2019s add a playlist feature to our music library: interface MusicTable : LogicalTable < MusicItem > { val playlistInfo : InlineView < PlaylistInfo . Key , PlaylistInfo > } data class PlaylistInfo ( @Attribute ( name = \"partition_key\" ) val playlist_token : String , val playlist_name : String , val playlist_version : Long , val track_tokens : List < AlbumTrack . Key > ) { @Attribute ( prefix = \"INFO_\" ) val sort_key : String = \"\" data class Key ( val playlist_token : String ) { val sort_key : String = \"\" } } To serialize writes to the same playlist, we can have all writers implement optimistic locking on the playlist_version attribute. private val musicTable : MusicTable fun changePlaylistName ( playlistToken : String , newName : String ) { // Read. val existing = musicTable . playlistInfo . load ( PlaylistInfo . Key ( playlistToken )) // Modify. val newPlaylist = existing . copy ( playlist_name = newName , playlist_version = existing . playlist_version + 1 ) // Write. musicTable . playlistInfo . save ( newPlaylist , ifPlaylistVersionIs ( existing . playlist_version ) ) } private fun ifPlaylistVersionIs ( playlist_version : Long ): DynamoDBSaveExpression { return DynamoDBSaveExpression () . withExpectedEntry ( \"playlist_version\" , ExpectedAttributeValue () . withComparisonOperator ( ComparisonOperator . EQ ) . withAttributeValueList ( AttributeValue (). withN ( \"$playlist_version\" )) ) }","title":"Optimistic Locking"},{"location":"guide/query_scan/","text":"Query \u00b6 In each DynamoDB table and its secondary indexes, items are grouped by partition key and sorted by the sort key. To query an index, you must provide the name of the partition key attribute and a single value for that attribute. Query returns all items with that partition key value. Optionally, you can provide a sort key attribute and use a comparison operator to refine the search results. Global secondary index queries cannot fetch attributes from the base table A projection is the set of attributes that is copied from a table into a secondary index. The partition key and sort key of the table are always projected into the index; you can project other attributes to support your application\u2019s query requirements. When you query an index, Amazon DynamoDB can access any attribute in the projection as if those attributes were in a table of their own. Let\u2019s continue with the music library example. interface MusicTable : LogicalTable < MusicItem > { val albumTracks : InlineView < AlbumTrack . Key , AlbumTrack > val albumTracksByTitle : SecondaryIndex < AlbumTrack . TitleIndexOffset , AlbumTrack > } data class AlbumTrack ( @Attribute ( name = \"partition_key\" ) val album_token : String , @Attribute ( name = \"sort_key\" , prefix = \"TRACK_\" ) val track_token : String , val track_title : String , val run_length : Duration ) { data class Key ( val album_token : String , val track_token : String = \"\" ) { constructor ( album_token : String , track_number : Long ) : this ( album_token , \"%016x\" . format ( track_number )) } @ForIndex ( \"album_track_title_index\" ) data class TitleIndexOffset ( val album_token : String , val track_title : String , // To uniquely identify an item in pagination. val track_token : String ? = null ) } Key Condition \u00b6 Partition Key and Entity Type \u00b6 This uses the primary index to find all tracks in the given album, sorted by track number. val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val page = musicTable . albumTracks . query ( keyCondition = BeginsWith ( prefix = AlbumTrack . Key ( albumToken ) ) ) return page . contents } Partition Key and Sort Key Prefix \u00b6 This uses the secondary index to find all tracks in the given album whose title starts with \u201cI want \u201c, sorted by title. val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val page = musicTable . albumTracksByTitle . query ( keyCondition = BeginsWith ( prefix = AlbumTrack . TitleIndexOffset ( albumToken , track_title = \"I want \" ) ) ) return page . contents } Partition Key and Sort Key Range \u00b6 This uses the primary index to find track 5 through 9 in the given album, sorted by track number. val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val page = musicTable . albumTracks . query ( keyCondition = Between ( startInclusive = AlbumTrack . Key ( albumToken , track_number = 5 ), endInclusive = AlbumTrack . Key ( albumToken , track_number = 9 )) ) return page . contents } Descending Order \u00b6 By default, the sort order is ascending. To reverse the order, set the asc parameter to false . val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val page = musicTable . albumTracks . query ( keyCondition = BeginsWith ( prefix = AlbumTrack . Key ( albumToken ) ), asc = false ) return page . contents } Filter Expression \u00b6 If you need to further refine the Query results, you can optionally provide a filter expression. A filter expression determines which items within the Query results should be returned to you. All of the other results are discarded. Filter expressions do not save cost A filter expression is applied after a Query finishes, but before the results are returned. Therefore, a Query consumes the same amount of read capacity, regardless of whether a filter expression is present. A Query operation can retrieve a maximum of 1 MB of data. This limit applies before the filter expression is evaluated. A filter expression cannot contain partition key or sort key attributes. You need to specify those attributes in the key condition expression, not the filter expression. This find all tracks in the given album that last longer than 3 minutes, sorted by track number. val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val page = musicTable . albumTracks . query ( keyCondition = BeginsWith ( prefix = AlbumTrack . Key ( albumToken )), filterExpression = runLengthLongerThan ( Duration . ofMinutes ( 3 )) ) return page . contents } private fun runLengthLongerThan ( duration : Duration ): FilterExpression { return FilterExpression ( \"run_length > :duration\" , mapOf ( \":duration\" to AttributeValue (). withS ( duration . toString ()) ) ) } Pagination \u00b6 val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val tracks = mutableListOf < AlbumTrack >() var page : Page < AlbumTrack . Key , AlbumTrack >? = null do { page = musicTable . albumTracks . query ( keyCondition = BeginsWith ( AlbumTrack . Key ( albumToken )), pageSize = 10 , initialOffset = page ?. offset ) tracks . addAll ( page . contents ) } while ( page ?. hasMorePages == true ) return tracks . toList () } Scan \u00b6 A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. val musicTable : MusicTable fun loadAllAlbumTracks (): List < AlbumTrack > { val page = musicTable . albumTracks . scan () return page . contents } Parallel Scan \u00b6 The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table\u2019s data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with an unique WorkerId . val musicTable : MusicTable suspend fun loadAllAlbumTracks (): List < AlbumTrack > { val segment1 = async { loadSegment ( 1 ) } val segment2 = async { loadSegment ( 2 ) } segment1 . await () + segment2 . await () } suspend fun loadSegment ( segment : Int ): List < AlbumTrack > { val page = musicTable . albumTracks . scan ( workerId = WorkerId ( segment , totalSegments = 2 ) ) return page . contents } Filter Expression \u00b6 See query filter expression above. Pagination \u00b6 See query pagination above.","title":"Query & Scan"},{"location":"guide/query_scan/#query","text":"In each DynamoDB table and its secondary indexes, items are grouped by partition key and sorted by the sort key. To query an index, you must provide the name of the partition key attribute and a single value for that attribute. Query returns all items with that partition key value. Optionally, you can provide a sort key attribute and use a comparison operator to refine the search results. Global secondary index queries cannot fetch attributes from the base table A projection is the set of attributes that is copied from a table into a secondary index. The partition key and sort key of the table are always projected into the index; you can project other attributes to support your application\u2019s query requirements. When you query an index, Amazon DynamoDB can access any attribute in the projection as if those attributes were in a table of their own. Let\u2019s continue with the music library example. interface MusicTable : LogicalTable < MusicItem > { val albumTracks : InlineView < AlbumTrack . Key , AlbumTrack > val albumTracksByTitle : SecondaryIndex < AlbumTrack . TitleIndexOffset , AlbumTrack > } data class AlbumTrack ( @Attribute ( name = \"partition_key\" ) val album_token : String , @Attribute ( name = \"sort_key\" , prefix = \"TRACK_\" ) val track_token : String , val track_title : String , val run_length : Duration ) { data class Key ( val album_token : String , val track_token : String = \"\" ) { constructor ( album_token : String , track_number : Long ) : this ( album_token , \"%016x\" . format ( track_number )) } @ForIndex ( \"album_track_title_index\" ) data class TitleIndexOffset ( val album_token : String , val track_title : String , // To uniquely identify an item in pagination. val track_token : String ? = null ) }","title":"Query"},{"location":"guide/query_scan/#key-condition","text":"","title":"Key Condition"},{"location":"guide/query_scan/#partition-key-and-entity-type","text":"This uses the primary index to find all tracks in the given album, sorted by track number. val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val page = musicTable . albumTracks . query ( keyCondition = BeginsWith ( prefix = AlbumTrack . Key ( albumToken ) ) ) return page . contents }","title":"Partition Key and Entity Type"},{"location":"guide/query_scan/#partition-key-and-sort-key-prefix","text":"This uses the secondary index to find all tracks in the given album whose title starts with \u201cI want \u201c, sorted by title. val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val page = musicTable . albumTracksByTitle . query ( keyCondition = BeginsWith ( prefix = AlbumTrack . TitleIndexOffset ( albumToken , track_title = \"I want \" ) ) ) return page . contents }","title":"Partition Key and Sort Key Prefix"},{"location":"guide/query_scan/#partition-key-and-sort-key-range","text":"This uses the primary index to find track 5 through 9 in the given album, sorted by track number. val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val page = musicTable . albumTracks . query ( keyCondition = Between ( startInclusive = AlbumTrack . Key ( albumToken , track_number = 5 ), endInclusive = AlbumTrack . Key ( albumToken , track_number = 9 )) ) return page . contents }","title":"Partition Key and Sort Key Range"},{"location":"guide/query_scan/#descending-order","text":"By default, the sort order is ascending. To reverse the order, set the asc parameter to false . val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val page = musicTable . albumTracks . query ( keyCondition = BeginsWith ( prefix = AlbumTrack . Key ( albumToken ) ), asc = false ) return page . contents }","title":"Descending Order"},{"location":"guide/query_scan/#filter-expression","text":"If you need to further refine the Query results, you can optionally provide a filter expression. A filter expression determines which items within the Query results should be returned to you. All of the other results are discarded. Filter expressions do not save cost A filter expression is applied after a Query finishes, but before the results are returned. Therefore, a Query consumes the same amount of read capacity, regardless of whether a filter expression is present. A Query operation can retrieve a maximum of 1 MB of data. This limit applies before the filter expression is evaluated. A filter expression cannot contain partition key or sort key attributes. You need to specify those attributes in the key condition expression, not the filter expression. This find all tracks in the given album that last longer than 3 minutes, sorted by track number. val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val page = musicTable . albumTracks . query ( keyCondition = BeginsWith ( prefix = AlbumTrack . Key ( albumToken )), filterExpression = runLengthLongerThan ( Duration . ofMinutes ( 3 )) ) return page . contents } private fun runLengthLongerThan ( duration : Duration ): FilterExpression { return FilterExpression ( \"run_length > :duration\" , mapOf ( \":duration\" to AttributeValue (). withS ( duration . toString ()) ) ) }","title":"Filter Expression"},{"location":"guide/query_scan/#pagination","text":"val musicTable : MusicTable fun loadAlbumTracks ( albumToken : String ): List < AlbumTrack > { val tracks = mutableListOf < AlbumTrack >() var page : Page < AlbumTrack . Key , AlbumTrack >? = null do { page = musicTable . albumTracks . query ( keyCondition = BeginsWith ( AlbumTrack . Key ( albumToken )), pageSize = 10 , initialOffset = page ?. offset ) tracks . addAll ( page . contents ) } while ( page ?. hasMorePages == true ) return tracks . toList () }","title":"Pagination"},{"location":"guide/query_scan/#scan","text":"A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. val musicTable : MusicTable fun loadAllAlbumTracks (): List < AlbumTrack > { val page = musicTable . albumTracks . scan () return page . contents }","title":"Scan"},{"location":"guide/query_scan/#parallel-scan","text":"The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table\u2019s data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with an unique WorkerId . val musicTable : MusicTable suspend fun loadAllAlbumTracks (): List < AlbumTrack > { val segment1 = async { loadSegment ( 1 ) } val segment2 = async { loadSegment ( 2 ) } segment1 . await () + segment2 . await () } suspend fun loadSegment ( segment : Int ): List < AlbumTrack > { val page = musicTable . albumTracks . scan ( workerId = WorkerId ( segment , totalSegments = 2 ) ) return page . contents }","title":"Parallel Scan"},{"location":"guide/query_scan/#filter-expression_1","text":"See query filter expression above.","title":"Filter Expression"},{"location":"guide/query_scan/#pagination_1","text":"See query pagination above.","title":"Pagination"},{"location":"guide/testing/","text":"You may test Tempset using DynamoDBLocal . Here is how we setup the docker container in our testing environment: DockerDynamoDb.kt .","title":"Testing"},{"location":"guide/transaction/","text":"Amazon DynamoDB transactions simplify the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications. Other regions could observe partial transactions Transactions are not supported across regions in global tables. For example, if you have a global table with replicas in the US East (Ohio) and US West (Oregon) regions and perform a TransactWriteItems operation in the US East (N. Virginia) Region, you may observe partially completed transactions in US West (Oregon) Region as changes are replicated. Changes will only be replicated to other regions once they have been committed in the source region. Transactional Read \u00b6 LogicalDb lets you load a consistent snapshot of up to 25 items in a transaction. val musicDb : MusicDb fun loadPlaylistTracks ( playlist : PlaylistInfo ) { val results = musicDb . transactionLoad ( playlist . track_tokens // [ AlbumTrack.Key(\"ALBUM_1\", track_number = 1), AlbumTrack.Key(\"ALBUM_354\", 12), AlbumTrack.Key(\"ALBUM_23\", 9) ] ) return results . getItems < AlbumTrack >() } Transactional Update \u00b6 LogicalDb lets you update, delete, and condition check up to 25 items atomically. The following example uses transactions to make sure that it only adds valid album tracks to the playlist. val musicDb : MusicDb val musicTable : MusicTable fun addTrackToPlaylist ( playlistToken : String , albumTrack : AlbumTrack . Key ) { // Read. val existing = musicTable . playlistInfo . load ( PlaylistInfo . Key ( playlistToken )) // Modify. val newPlaylist = existing . copy ( playlist_tracks = existing . playlist_tracks + albumTrack , playlist_version = existing . playlist_version + 1 ) // Write. val writeSet = TransactionWriteSet . Builder () . save ( newPlaylist , ifPlaylistVersionIs ( existing . playlist_version )) // Add a playlist entry only if the album track exists. . checkCondition ( albumTrack , trackExists ()) . build () musicDb . transactionWrite ( writeSet ) } private fun ifPlaylistVersionIs ( playlist_version : Long ): DynamoDBTransactionWriteExpression { return DynamoDBTransactionWriteExpression () . withConditionExpression ( \"playlist_version = :playlist_version\" ) . withExpressionAttributeValues ( mapOf ( \":playlist_version\" to AttributeValue (). withN ( \"$playlist_version\" ) ) ) } private fun trackExists (): DynamoDBTransactionWriteExpression { return DynamoDBTransactionWriteExpression () . withConditionExpression ( \"attribute_exists(track_title)\" ) } Writing Pager \u00b6 To make the 25 item limit easier to work with, we created WritingPager , a control flow abstraction for paging transactional writes. The following example splits the write into multiple transactions if there are more than 25 tracks. val musicDb : MusicDb val musicTable : MusicTable fun addTracksToPlaylist ( playlistToken : String , albumTracks : List < AlbumTrack . Key > ) { musicDb . transactionWritingPager ( albumTracks , maxTransactionItems = 25 , handler = AlbumTrackWritingPagerHandler ( playlistToken , musicTable ) ). execute () } class AlbumTrackWritingPagerHandler ( private val playlistToken : String , private val musicTable : MusicTable ) : WritingPager . Handler < AlbumTrack . Key > { private lateinit var currentPagePlaylistInfo : PlaylistInfo private lateinit var currentPageTracks : List < AlbumTrack . Key > override fun eachPage ( proceed : () -> Unit ) { proceed () } override fun beforePage ( remainingUpdates : List < AlbumTrack . Key >, maxTransactionItems : Int ): Int { // Reserve 1 for the playlist info at the end. currentPageTracks = remainingUpdates . take (( maxTransactionItems - 1 )) currentPagePlaylistInfo = musicTable . playlistInfo . load ( PlaylistInfo . Key ( playlistToken )) !! return currentPageTracks . size } override fun item ( builder : TransactionWriteSet . Builder , item : AlbumTrack . Key ) { builder . checkCondition ( item , trackExists ()) } override fun finishPage ( builder : TransactionWriteSet . Builder ) { val existing = currentPagePlaylistInfo val newPlaylist = existing . copy ( playlist_tracks = existing . playlist_tracks + currentPageTracks , playlist_version = existing . playlist_version + 1 ) builder . save ( newPlaylist , ifPlaylistVersionIs ( existing . playlist_version )) } }","title":"Transaction"},{"location":"guide/transaction/#transactional-read","text":"LogicalDb lets you load a consistent snapshot of up to 25 items in a transaction. val musicDb : MusicDb fun loadPlaylistTracks ( playlist : PlaylistInfo ) { val results = musicDb . transactionLoad ( playlist . track_tokens // [ AlbumTrack.Key(\"ALBUM_1\", track_number = 1), AlbumTrack.Key(\"ALBUM_354\", 12), AlbumTrack.Key(\"ALBUM_23\", 9) ] ) return results . getItems < AlbumTrack >() }","title":"Transactional Read"},{"location":"guide/transaction/#transactional-update","text":"LogicalDb lets you update, delete, and condition check up to 25 items atomically. The following example uses transactions to make sure that it only adds valid album tracks to the playlist. val musicDb : MusicDb val musicTable : MusicTable fun addTrackToPlaylist ( playlistToken : String , albumTrack : AlbumTrack . Key ) { // Read. val existing = musicTable . playlistInfo . load ( PlaylistInfo . Key ( playlistToken )) // Modify. val newPlaylist = existing . copy ( playlist_tracks = existing . playlist_tracks + albumTrack , playlist_version = existing . playlist_version + 1 ) // Write. val writeSet = TransactionWriteSet . Builder () . save ( newPlaylist , ifPlaylistVersionIs ( existing . playlist_version )) // Add a playlist entry only if the album track exists. . checkCondition ( albumTrack , trackExists ()) . build () musicDb . transactionWrite ( writeSet ) } private fun ifPlaylistVersionIs ( playlist_version : Long ): DynamoDBTransactionWriteExpression { return DynamoDBTransactionWriteExpression () . withConditionExpression ( \"playlist_version = :playlist_version\" ) . withExpressionAttributeValues ( mapOf ( \":playlist_version\" to AttributeValue (). withN ( \"$playlist_version\" ) ) ) } private fun trackExists (): DynamoDBTransactionWriteExpression { return DynamoDBTransactionWriteExpression () . withConditionExpression ( \"attribute_exists(track_title)\" ) }","title":"Transactional Update"},{"location":"guide/transaction/#writing-pager","text":"To make the 25 item limit easier to work with, we created WritingPager , a control flow abstraction for paging transactional writes. The following example splits the write into multiple transactions if there are more than 25 tracks. val musicDb : MusicDb val musicTable : MusicTable fun addTracksToPlaylist ( playlistToken : String , albumTracks : List < AlbumTrack . Key > ) { musicDb . transactionWritingPager ( albumTracks , maxTransactionItems = 25 , handler = AlbumTrackWritingPagerHandler ( playlistToken , musicTable ) ). execute () } class AlbumTrackWritingPagerHandler ( private val playlistToken : String , private val musicTable : MusicTable ) : WritingPager . Handler < AlbumTrack . Key > { private lateinit var currentPagePlaylistInfo : PlaylistInfo private lateinit var currentPageTracks : List < AlbumTrack . Key > override fun eachPage ( proceed : () -> Unit ) { proceed () } override fun beforePage ( remainingUpdates : List < AlbumTrack . Key >, maxTransactionItems : Int ): Int { // Reserve 1 for the playlist info at the end. currentPageTracks = remainingUpdates . take (( maxTransactionItems - 1 )) currentPagePlaylistInfo = musicTable . playlistInfo . load ( PlaylistInfo . Key ( playlistToken )) !! return currentPageTracks . size } override fun item ( builder : TransactionWriteSet . Builder , item : AlbumTrack . Key ) { builder . checkCondition ( item , trackExists ()) } override fun finishPage ( builder : TransactionWriteSet . Builder ) { val existing = currentPagePlaylistInfo val newPlaylist = existing . copy ( playlist_tracks = existing . playlist_tracks + currentPageTracks , playlist_version = existing . playlist_version + 1 ) builder . save ( newPlaylist , ifPlaylistVersionIs ( existing . playlist_version )) } }","title":"Writing Pager"}]}